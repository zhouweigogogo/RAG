中国人工智能系列白皮书——大模型技术（2023版）
中国人工智能学会
二○二三年九月
《中国人工智能系列白皮书----大模型技术》编写组
陶建华  吴  飞  黄民烈  文继荣  王海峰  刘知远
刘  静  杨小康  聂  帅
大模型技术概述
第1 章 大模型技术概述
1.1 大模型技术的发展历程
1.2 大模型技术的生态发展
1.3 大模型技术的风险与挑战
语言大模型技术
第2 章 语言大模型技术
2.1 Transformer 架构
2.2 语言大模型架构
2.2.1 掩码语言建模
2.2.2 自回归语言建模
2.2.3 序列到序列建模
2.3 语言大模型关键技术
2.3.1 语言大模型的预训练
2.3.2 语言大模型的适配微调
2.3.3 语言大模型的提示学习
2.3.4 语言大模型的知识增强
2.3.5 语言大模型的工具学习
多模态大模型技术
第3 章 多模态大模型技术
3.1 多模态大模型的技术体系
3.1.1 面向理解任务的多模态大模型
3.1.2 面向生成任务的多模态大模型
3.1.3 兼顾理解和生成任务的多模态大模型
3.1.4 知识增强的多模态大模型
3.2 多模态大模型的关键技术
3.2.1 多模态大模型的网络结构设计
以上内容去除了页码、目录编号等冗余信息，并保留了文章的流畅性和通顺性。
```
第3章 多模态大模型的自监督学习优化
第3章 多模态大模型的下游任务微调适配
第4章 大模型技术生态
4.1 典型大模型平台
4.2 典型开源大模型
4.2.1 典型开源语言大模型
4.2.2 典型开源多模态大模型
4.3 典型开源框架与工具
4.4 大模型的训练数据
4.4.1 大模型的训练数据处理流程和特点
4.4.2 大模型常用的公开数据集
第5章 大模型的开发训练与推理部署
5.1 大模型开发与训练
5.2 大模型推理部署
5.2.1 大模型压缩
5.2.2 大模型推理与服务部署
5.3 软硬件适配与协同优化
5.3.1 大模型的软硬件适配
5.3.2 大模型的软硬件协同优化
第6章 大模型应用
6.1 信息检索
6.2 新闻媒体
6.3 智慧城市
6.4 生物科技
6.5 智慧办公
6.6 影视制作
6.7 智能教育
```
智慧金融、智慧医疗、智慧工厂、生活服务、智能机器人、其他应用。
第7章 大模型的安全性
7.1 大模型安全风险引发全球广泛关注
7.2 大模型安全治理的政策法规和标准规范
7.3 大模型安全风险的具体表现
   7.3.1 大模型自身的安全风险
   7.3.2 大模型在应用中衍生的安全风险
7.4 大模型安全研究关键技术
   7.4.1 大模型的安全对齐技术
   7.4.2 大模型安全性评测技术
第8章  总结与思考
8.1 协同多方合作，共同推动大模型发展
8.2 建立大模型合规标准和评测平台
8.3 应对大模型带来的安全性挑战
8.4 开展大模型广泛适配，推动大模型技术栈自主可控
大模型技术概述
1.1 大模型技术的发展历程
2006年，Geoffrey Hinton提出通过逐层无监督预训练的方式来缓解由于梯度消失而导致的深层网络难以训练的问题，为神经网络的有效学习提供了重要的优化途径。此后，深度学习在计算机视觉、语音、自然语言处理等众多领域取得了突破性的研究进展，开启了新一轮深度学习的发展浪潮。
总结过去十多年的技术发展，基于深度学习的人工智能技术主要经历了如下研究范式转变：从早期的“标注数据监督学习”的任务特定模型，到“无标注数据预训练+标注数据微调”的预训练模型，再到如今的“大规模无标注数据预训练+指令微调+人类对齐”的大模型，经历了从小数据到大数据，从小模型到大模型，从专用到通用的发展历程，人工智能技术正逐步进入大模型时代。
2022年底，由OpenAI发布的语言大模型ChatGPT引发了社会的广泛关注。在“大模型+大数据+大算力”的加持下，ChatGPT能够通过自然语言交互完成多种任务，具备了多场景、多用途、跨学科的任务处理能力。以ChatGPT为代表的大模型技术可以在经济、法律、社会等众多领域发挥重要作用。大模型被认为很可能像PC时代的操作系统一样，成为未来人工智能领域的关键基础设施，引发了大模型的发展热潮。
本次大模型热潮主要由语言大模型（亦称为大语言模型）引领。语言大模型通过在海量无标注数据上进行大规模预训练，能够学习到大量的语言知识与世界知识，并且通过指令微调、人类对齐等关键技术拥有面向多任务的通用求解能力。在原理上，语言大模型旨在构建面向文本序列的概率生成模型，其发展过程主要经历了四个主要阶段。
统计语言模型主要基于马尔可夫假设建模文本序列的生成概率。特别地，N-gram 语言模型认为下一个词汇的生成概率只依赖于前面出现的N个词汇。此类语言模型的问题在于容易受到数据稀疏问题的影响，需要使用平滑策略改进概率分布的估计，对于文本序列的建模能力较弱。
神经语言模型针对统计语言模型存在的问题，主要通过神经网络（如MLP、RNN）建模目标词汇与上下文词汇的语义共现关系，能够有效捕获复杂的语义依赖关系，更为精准建模词汇的生成概率。word2vec简化了神经语言模型的网络架构，可以从无监督语料中学习可迁移的词表示（又称为词向量或词嵌入），为后续预训练语言模型的研究奠定了基础。
预训练语言模型基于“预训练+微调”的学习范式构建，首先通过自监督学习任务从无标注文本中学习可迁移的模型参数，进而通过有监督微调适配下游任务。早期的代表性预训练语言模型包括ELMo、GPT-1和BERT等。其中，ELMo模型基于传统的循环神经网络（LSTM）构建，存在长距离序列建模能力弱的问题；随着Transformer的提出，神经网络序列建模能力得到了显著的提升，GPT-1和BERT都是基于Transformer架构构建的，可通过微调学习解决大部分的自然语言处理任务。
在预训练语言模型的研发过程中，一个重要的经验性法则是扩展定律：随着模型参数规模和预训练数据规模的不断增加，模型能力与任务效果将会随之改善。OpenAI在研发GPT系列模型过程中，主要探索了GPT-1（1.1亿参数）、GPT-2（15亿参数）以及GPT-3（1750亿参数）三个不同参数规模的模型，谷歌也推出了参数规模高达5400亿参数的PaLM模型。当模型参数规模达到千亿量级，语言大模型进入了探索阶段。
能够展现出多方面的能力跃升。例如，GPT-3 在没有微调的情况下，可以仅通过提示词或少数样例（上下文学习）完成多种任务，甚至在某些任务上超过当时最好的专用模型。学术界引入了“语言大模型”来特指这种超大规模的预训练语言模型，以突出与早期预训练语言模型的不同。
针对这些问题，主要有两种大模型改进技术，包括指令微调以及基于人类反馈的强化学习。指令微调利用格式化（指令和回答配对）的训练数据加强大模型的通用任务泛化能力；基于人类反馈的强化学习将人类标注者引入到大模型的学习过程中，训练与人类偏好对齐的奖励模型，进而有效指导语言大模型的训练，使得模型能够更好地遵循用户意图，生成符合用户偏好的内容。在大模型使用过程中，可以使用各种提示技术（包括思维链、思维树等），从而更好地利用大模型的潜在能力。
力，提升大模型解决实际问题的能力。进一步，语言大模型主要是基于文本数据形式进行训练与推理，存在一些特定能力的不足，例如数值计算等。针对这一问题，可以使用外部工具（如计算器、搜索引擎等）扩展大模型的能力边界。
作为重要前沿探索力量，OpenAI 对于语言大模型的研发工作主要是在Transformer 架构推出后开展，形成了一系列的技术进展。其中，GPT-1 探索了解码器Transformer 架构（decoder-only Transformer）在“预训练+微调”范式下的自然语言任务求解能力；GPT-2 初步验证了扩大模型参数规模的有效性（扩展法则），并且探索了基于自然语言提示的多任务解决能力；GPT-3 首次探索了千亿参数规模的语言模型效果，提出了基于“上下文学习”的任务解决方法；CodeX 使用代码数据对GPT-3 进行微调，从而提升代码能力和复杂推理能力；InstructGPT 基于人类反馈的强化学习技术（RLHF），能够强化对于人类指令的遵循能力和人类偏好的对齐能力；ChatGPT 与InstructGPT 的技术原理相似，进一步引入了对话数据进行学习，从而加强了多轮对话能力；GPT-4 能够处理更长的上下文窗口，具备多模态理解能力，在逻辑推理、复杂任务处理方面的能力得到显著。
随着GPT-4的成功，语言大模型对于多模态领域也产生了重要影响，它从单调的文本交互，升级为可以接受文本与图像组合的多模态输入。相比传统的单模态大模型，多模态大模型更加符合人类的多渠道感认知方式，能够应对更加复杂丰富的环境、场景和任务。GPT-4表明在多模态大模型中引入基于人类知识的自然语言能够带来模型在多模态理解、生成、交互能力上的提升。
大模型服务平台正向个人开放及商业落地应用延伸，不同公司各有侧重，为用户提供了多种获取大模型能力的途径。OpenAI API较早地面向公众开放的大模型服务平台，用户可以通过API访问不同的GPT模型来完成下游任务。Claude系列模型是由Anthropic开发的闭源语言大模型，目前包含Claude和Claude-Instant两种模型可供选择。该系列模型通过无监督预训练、基于人类反馈的强化学习和Constitutional AI技术进行训练，旨在改进模型的有用性、诚实性和无害性。Claude最高支持100K词元的上下文，而Claude-2更是拓展到了200K词元的上下文。文心一言是基于百度文心大模型的知识增强语言大模型，提供APP、网页版、API接口等多种形式的开放服务。文心一言还建设了插件机制，通过外部工具、服务的调用，拓展大模型的能力的边界。讯飞星火认知大模型具有开放式知识问答、多轮对话、逻辑和数学能力，并且具有较强的对代码和多模态的理解能力。讯飞和华为还联合重磅发布了国内首款支持大模型训练私有化的全国产化产品“星火一体机”，可支持企业快速实现讯飞星火大模型的私有化部署、场景赋能和专属大模型训练优化。
大模型的开源生态也“百花齐放”，主要包括开源框架与开源大模型。开源框架可以有效地支撑大规模模型的训练，如：PyTorch。
提供了分桶梯度、通信计算重叠、跳过同步等技术，支持大规模的分布式数据并行训练；飞桨是国产的深度学习框架，早在内部就支持了大规模分布式训练，覆盖了计算机视觉、自然语言处理等多个领域的模型，其中4D混合并行策略，可训练千亿规模模型；OneFlow将分布式集群抽象成逻辑上的超级设备，支持动静态图灵活转换，以数据+模型混合并行提升性能；DeepSpeed是微软推出的大模型训练框架，其中ZeRO技术减少冗余内存访问，使得可以训练万亿级模型。开源大模型可降低大模型研究的门槛，促进大模型应用的繁荣。其中典型代表有：LLaMA系列是Meta研发的开源大模型，参数规模从7B到65B不等，仅依赖公开数据集进行预训练，通过数据过滤和并行优化实现高效训练。Falcon系列来自阿布扎比的TII研究院，最大规模达180B参数，基于开源许可发布，性能与GPT-4和PaLM2相当，参数量却较小。GLM系列采用空白填充等多任务联合训练方式，提升了模型的生成能力。Baichuan系列模型由百川智能开发，支持中英双语，使用高质量训练数据，在多个基准测试上表现优秀，该系列模型还开源了多种量化版本。Baichuan 2在保留原有模型优势的基础上，增强了逻辑推理等方面的能力。CPM系列采用经典的语言模型自回归训练方式，在各类中文NLP任务上均表现卓越。
大模型技术具有广泛的应用场景，可以用来赋能不同行业。大模型+传媒可以实现智能新闻写作，降低新闻的生产成本；大模型+影视可以拓宽创作素材，开拓创作思路，激发创作灵感，提升作品质量；大模型+营销可以打造虚拟客服，助力产品营销；大模型+娱乐可以加强人机互动，激发用户参与热情，增加互动的趣味性和娱乐性；大模型+军事可以增强军事情报和决策能力，可以实现实时战场翻译，快速准确的威胁评估、作战任务规划和执行、战场感知、战术决策支持、改进态势感知等；大模型+教育可以赋予教育教材新活力，让教育方式更个性化、更智能；大模型+金融可以帮助金融机构降本增效。
金融服务更有温度；大模型+医疗可以赋能医疗机构诊疗全过程。总之，大模型的发展将给人类带来了非常强大的助推力，让数字世界和现实世界的共生变得更为便捷、更为有效。大模型的通用性使其被认为是可以成为未来人工智能应用中的关键基础设施，就像PC时代的操作系统一样，赋能百业，加速推进国民经济的高质量发展。向上，大模型可带动上游软硬件计算平台的革新，形成高性能软硬件与大模型的协同发展，构建“大模型+软硬件+数据资源”上游发展生态；向下，大模型可以打造“大模型+应用场景”的下游应用生态，加速全产业的智能升级，对经济、社会和安全等领域的智能化升级中形成关键支撑。
1.3 大模型技术的风险与挑战
尽管以ChatGPT为代表的大模型技术取得关键性突破，但当前大模型技术仍存在诸多风险与挑战。
首先，大模型的可靠性无法得到有效保障。例如，基于海量数据训练的语言大模型，尽管其生成的内容符合语言规则、通顺流畅且与人类偏好对齐，但其合成内容在事实性、时效性方面等仍存在较多问题，尚无法对所合成内容做出可靠评估。
其次，大模型的可解释性存在不足。大模型基于深度神经网络，为黑盒模型，其工作机理仍难以理解。语言大模型的涌现能力、规模定律，多模态大模型的知识表示、逻辑推理能力、泛化能力、情景学习能力等方面有待展开深入研究，为大模型的大规模实际应用提供理论保障。
再次，大模型应用部署代价高。大模型参数规模和数据规模都非常巨大，存在训练和推理计算量大、功耗高、应用成本高、端侧推理存在延迟等问题，从而限制了其落地应用。提高推理速度降低大模型使用成本是大规模应用的关键。
此外，大模型在小数据情景下的迁移能力存在不足。
数据驱动深度学习方式，依赖训练数据所覆盖的场景，由于复杂场景数据不足，大模型存在特定场景适用性不足的问题，面临鲁棒性和泛化性等挑战。提升大模型对小数据的高效适配迁移能力是未来研究的重点。最后，大模型还存在伴生技术风险问题。例如，语言大模型具有通用的自然语言理解和生成能力，其与语音合成、图像视频生成等技术结合可以产生人类难以辨别的音视频等逼真多媒体内容，可能会被滥用于制造虚假信息、恶意引导行为，诱发舆论攻击、甚至危害国家安全。此外，大模型存在安全与隐私问题，目前针对大模型安全漏洞的典型攻击方式包括：数据投毒攻击、对抗样本攻击、模型窃取攻击、后门攻击、指令攻击。大模型的安全漏洞可能被攻击者利用，使得大模型关联业务面临整体失效的风险，威胁以其为基础构建的应用生态。大模型利用海量的互联网数据进行训练，包括个人、企业甚至国家的敏感数据可能被编码进大模型参数中，因而存在数据隐私问题。例如，通过提示信息可能诱发大模型隐私数据泄露问题。
近年来，在Transformer架构基础上构建的预训练语言模型为自然语言处理领域带来了一系列突破式进展，成为人工智能主流技术范式。预训练语言模型采用“预训练+微调”方法，主要分为两步：1）将模型在大规模无标注数据上进行自监督训练得到预训练模型，2）将模型在下游各种自然语言处理任务上的小规模有标注数据进行微调得到适配模型。由于预训练语言模型参数越大模型表现越好，这激发了语言大模型（Large Language Model, LLM）研究热潮。
Transformer架构是目前语言大模型采用的主流架构，基于自注意力机制(Self-attention Mechanism)模型。其主要思想是通过自注意力机制获取输入序列的全局信息，并将这些信息通过网络层进行传递。标准的Transformer是一个编码器-解码器架构，其编码器和解码器均由一个编码层和若干相同的Transformer模块层堆叠组成。编码器的Transformer模块层包括多头注意力层和全连接前馈网络层，这两部分通过残差连接和层归一化操作连接起来。与编码器模块相比，解码器由于需要考虑解码器输出作为背景信息进行生成，其中每个Transformer层多了一个交叉注意力层。相比于传统循环神经网络（Recurrent Neural Network, RNN）和长短时记忆神经网络（Long Short-Term Memory Network, LSTM），Transformer架构的优势在于它的并行计算能力，即不需要按照时间步顺序地进行计算。
Transformer架构包含编码层与Transformer模块两个核心组件。编码层，主要是将输入词序列映射到连续值向量空间进行编码，每个词编码由词嵌入和位置编码构成，由二者加和得到：1）词嵌入，在Transformer架构中，词嵌入是输入数据的第一步处理过程，它将词映射到高维空间中的向量，可以捕获词汇的语义信息，如词义和语法关系。每个词都被转化为一个固定长度的向量。
为了让模型能够理解序列中的顺序信息，引入了位置编码。标准Transformer架构的位置编码方式是使用正弦和余弦函数的方法。对于每个位置i，对应的位置编码是一个长度为d的向量，其中d是模型的嵌入维度。这个向量的第j个元素由以下公式计算：如果j是偶数，那么编码的第j个元素为sin(πj/d); 如果j是奇数，那么编码的第j个元素为cos(πj/d)。
Transformer 模块通过自注意力机制获取输入序列的全局信息，并将这些信息通过网络层进行传递，包括多头注意力层和全连接前馈网络层。这两部分通过残差连接和层归一化操作连接起来，由自注意力层、全连接前馈层、残差连接和层归一化操作等基本单元组成。
自注意力层是Transformer 模型的核心组成部分，包含查询矩阵Q、键矩阵K和值矩阵V，其中矩阵的每一行对应一个词。注意力机制的直观理解是，矩阵H中的每一行是V中行向量的加权和，权重由查询向量和键矩阵的点积决定。
具有序列长度n的查询序列的特征矩阵记为Xq，具有序列长度m的键-值序列的特征矩阵记为Xkv，三个矩阵Q、K、V由三个线性变换得到。Transformer 模型采用的特定注意力机制被称为自注意力机制，因为三个矩阵Q、K、V都来自于前一层的相同输入。
Transformer采用了多头自注意力机制，即输入序列被线性映射多次得到不同的投影矩阵。多头注意力生成多个高维的注意力表示，这使得其比单头注意力具有更强的表达能力。
对于解码器，Transformer层在Attention的Softmax之前引入了一个额外的掩码（MASK）操作，防止查询矩阵Q对序列中尚未解码的后续位置施加注意力操作。此外，在自注意层之后还有一个额外的“交叉注意力”层。
全连接前馈层，在注意力层之后的全连接前馈层由两个线性变换和一个非线性激活函数组成。将输入矩阵表示为\( X \in \mathbb{R}^{n \times d} \)，前馈层的输出为：
\[ FFN(X) = \sigma(XW_1 + b_1)W_2 + b_2 \]
其中，\( \sigma \)是激活函数（通常为ReLU或GELU），而\( W_1, b_1, W_2, b_2 \)均为可学习的参数。在实践中，通常设置\( d \)为输入维度，而\( f \)（隐层维度）设置为输入维度的4倍。FFN的作用包括两个方面：（1）非线性激活：在每个注意力模块之后引入了非线性激活函数，这有助于增强模型的表达能力；（2）信息整合：自注意力机制允许模型在不同的位置整合信息。
位置间建立联系，而全连接前馈网络则在每个位置独立地对信息进行整合，这两者结合起来，使得模型既能捕获全局（长距离）的信息，又能在每个位置进行局部的信息整合。
残差连接和层归一化，在每个注意力层和每个全连接前馈层之后，Transformer 都应用残差连接和层归一化技术，这有助于在模型非常深时保留信息并确保模型性能。
在Transformer 模型被提出之后，它也衍生出了相当一部分的变体，包括在编码器和解码器中出现了不同方式的注意力机制、归一化操作、残差连接、前馈层和位置编码等。
现有的语言大模型几乎全部是以 Transformer 模型作为基础架构来构建的，不过它们在所采用的具体结构上通常存在差异，如只使用 Transformer 编码器或解码器，或者同时使用两者。
从建模策略的角度，语言大模型架构大致可以分为三类。
2.2.1 掩码语言建模
掩码语言建模是基于 Transformer 编码器的双向模型，其中 BERT和 RoBERTa是其中典型代表。这类模型通过掩码语言建模任务进行预训练。
中还加入了下一句预测（Next Sentence Prediction, NSP）任务。在预训练时，模型的输入是自然语言序列。首先在原始输入中添加特殊标记 [CLS] 和 [SEP]，并且随机用特殊标记替换原始序列中的字符。掩码语言建模旨在根据上下文来最大化位置标签字符的条件概率，即让模型执行“完型填空”任务。而 [CLS] 的最终表示被用于预测两个句子是否连贯。RoBERTa 与 BERT 基本相同，但是它删去了下一句预测任务，采用了更具鲁棒性的动态掩码机制，并使用更大的批次、更长的时间和更多的数据进行训练。
自回归语言建模自回归语言模型在训练时通过学习预测序列中的下一个词来建模语言，其主要是通过Transformer 解码器来实现。自回归语言模型的优化目标为最大化对序列中每个位置的下一个词的条件概率的预测。代表性模型，包括OpenAI 的GPT 系列模型、Meta 的LLaMA 系列模型和Google 的PaLM 系列模型。其中，GPT-3 是首个将模型参数扩增到千亿参数规模的预训练模型。自回归语言模型更加适用于生成任务，同时也更适用于对模型进行规模扩增。
序列到序列建模序列到序列模型是建立在完整Transformer 架构上的序列到序列模型，即同时使用编码器-解码器结构，代表性模型包括T5和BART。这两个模型都采用文本片段级别的掩码语言模型作为主要的预训练任务，即随机用单个特殊标记替换文本中任意长度的一段字符序列，并要求模型生成填充原始的字符。序列到序列模型可以形式化地表示为最大化在给定掩码的字符序列的情况下目标字符序列的概率。
总体而言，自回归语言模型较其它预训练语言模型架构展现了更优异的情境学习、思维链推理、内容创造等能力，自回归模型架构是当前大模型的主流架构。
语言大模型技术主要包括模型预训练、适配微调、提示学习、知识增强和工具学习等。支撑语言大模型高效训练的技术主要有高性能训练工具、高效预训练策略、高质量训练数据、高效的模型架构等。
高效预训练策略的主要思路是采用不同的策略以更低成本实现对语言大模型的预训练。这包括设计高效的优化任务目标，使得模型能够利用每个样本更多的监督信息，实现模型训练的加速。热启动策略通过线性地提高学习率来解决预训练中可能出现的优化困难。渐进式训练策略则是先训练浅层模型，再复制构建深层模型，认为不同的层可以共享相似的自注意力模式。知识继承方法在模型训练中同时学习文本和已预训练语言大模型中的知识，以加速模型训练。在中文语言大模型CPM-2中，采用知识继承技术可以使大模型在预训练前期提速37.5%。可预测扩展策略旨在大模型训练初期，利用大模型和小模型的同源性关系，通过拟合较小模型的性能曲线预测大模型性能，指导大模型训练优化。
OpenAI在GPT-4训练中，使用较少计算资源训练的小模型可靠地预测GPT-4某些性能，大幅降低了模型训练成本。
高效的模型架构。BERT 之后的Transformer 架构在提高自然语言处理效率方面有两个重要优化方向：
（1）统一的序列建模，旨在将多种自然语言处理任务（如分类、信息抽取、翻译、对话等）整合到一个统一的框架，然后在同一模型中执行多个任务，以实现更高效的 natural language processing。该方法可以充分利用大规模训练数据，从而提高了模型在多个任务上的性能和泛化性。这减少了开发和维护多个单独模型的复杂性以及资源消耗，提高模型的通用性。统一任务序列建模有两种方式：一是转化为序列生成的统一任务，如T5[42]和BART[43]等将多种自然语言任务统一转化文本到文本的生成任务；二是转化为语言大模型预训练任务，通过语言提示在输入文本中插入人类设计或者自动生成的上下文，实现对不同任务的处理。
（2）计算高效的模型架构。从Transformer 模型架构本身在处理训练复杂度、编解码效率、训练稳定性、显存利用等方面进行优化。比如，Transformer 其并行处理机制是以低效推理为代价的，解码时每个步骤的复杂度为O(N)，Transformer 模型也是显存密集型模型，输入序列越长、占用的内存越多。为此，微软设计了一种新的Transformer 架构RetNet[44]，其采用线性化注意力+尺度保持（Retention ）机制，在基本保持模型性能的基础上同时实现模型训练速度、推断速度和内存节约的大幅提升。针对自注意力显存消耗大，斯坦福大学在Transformer 中引入FashAttention[45]，给出了一种具有IO 感知，且兼具快速、内存高效的优化方法。
的注意力算法，已经被各种主流大模型采用以扩展对超长文本输入的支持。最近，模块化大模型架构引起广泛关注，其利用大模型的神经激活稀疏性，对稠密模型进行模块化划分，不同任务只经过部分模块计算实现训练和推理加速，典型工作包括Google的Switch Transformers和Pathways架构、清华大学的MoEfication架构、FastMoE架构等。
语言大模型由于在大规模通用领域数据预训练通常缺乏对特定任务或领域的知识，因此需要适配微调。微调可以帮助模型更好地适应特定需求，如对敏感数据（如医疗记录）的处理，同时不暴露原始数据。此外，微调可以提高部署效率、减少计算资源需求。指令微调和参数高效学习是适配微调的关键技术。
指令微调/Instruction Tuning，是一种可以帮助语言大模型实现人类语言指令遵循的能力，在零样本设置中泛化到未见任务上的学习方法。指令微调学习形式与多任务提示微调相似，但与提示微调让提示适应语言大模型并且让下游任务对齐预训练任务不同，其是让语言大模型对齐理解人类指令并按照指令要求完成任务，即在给定指令提示的情况下给出特定的回应，其中提示可以选择性包含一条解释。
任务的指令。指令微调研究涉及指令理解、指令数据获取和指令对齐等内容。
（1）指令理解，指语言大模型准确理解人类语言指令的能力，是语言大模型执行指令完成任务的前提。为了增强对指令的理解，许多工作采用多任务提示方式对基于指令描述的大量任务集上对语言大模型进行微调，如FLAN[50]、InstructGPT[21]等，这些模型在未见任务上显示出优越的零样本性能。
（2）指令数据获取，指如何构建包含多样性的任务指令数据。指令数据构建常见有三种方式：i）基于公开人工标注数据构建，代表指令数据集包括1616种不同任务的Super-Natural Instruction[51]、2000种不同NLP任务的OPT-IML[52]。ii）借助语言大模型的自动生成构建，如Unnatural Instructions[53]，通过种子指令作为提示让语言大模型生成新的指令描述和问题，然后再输入到模型让其输出回答。iii）基于人工标注方法，如ChatGPT在人工标注指令的基础上通过GPT-3、InstructGPT等在线平台收集用户真实指令数据。
（3）指令对齐，语言大模型在多种自然语言处理任务上都展现了卓越的性能。然而，它们有时可能会出现不预期的行为，如创造虚假信息、追求错误目标或产生有偏见的内容[5]。其根本原因在于，语言大模型在预训练时仅通过语言模型建模，未涉及人类的价值观或偏好。为了解决这一问题，研究者提出了“指令对齐”，使语言大模型的输出更符合人类的预期。但这种对齐与原始预训练有所不同，更注重于有用性、诚实性和无害性。此外，指令对齐可能会降低语言大模型的某些通用能力，这被称为“Alignment Tax”。为实现模型输出与对人类价值的对齐，InstructGPT提出了一种基于人类反馈的微调方法，利用了强化学习技术，将人类反馈纳入模型微调过程。实际上，ChatGPT也采用了与InstructGPT相似的技术，以确保产生高质量且无害的输出。指令对齐的广泛应用，适配微调从纯数据学习的传统微调。
调范式开始逐步向人类学习范式的转变。参数高效微调（Parameter-Efficient Tuning）。早期以BERT为代表的微调方法，是在大模型基座上增加一个任务适配层，然后进行全参微调。但是这种方法存在两方面的问题：一是任务“鸿沟”问题，预训练和微调之间的任务形式不一致，这种差别会显著影响知识迁移的效能。二是高计算成本，语言大模型的参数规模不断增长，导致模型全参微调也需要大量计算资源。
解决以上问题的有效途径是参数高效学习，即通过仅微调少量参数实现大模型在下游任务上获得全参微调效果。目前许多参数高效微调方法被提出，这些方法大致可分为3类：
（1）添加式方法：旨在原模型基础上引入额外的模块或参数，并仅微调该引入部分的参数。如适配器（Adapter）方法，将小规模的神经模块（适配器）注入到预训练模型中，并只调整这些适配器以进行模型自适应。在实际应用中，适配器模块通常分别插入在多头自注意和前馈网络子层之后，成为最广泛使用方式。
（2）指定式方法：旨在原模型指定模型中部分参数为可训练参数，并固定模型其他参数。这类方法简单也十分有效，如仅通过优化模型内的偏置项并固定其他参数，模型仍然可以再现95%以上的模型全参微调性能。
（3）重参数化方法：将原模型或部分模型参数重参数化到低维度参数空间中，仅仅优化低维空间中的近似参数，显著降低模型的计算量和内存消耗。如LoRA，将模型自注意力模块的变化权重参数分解为两个低秩矩阵相乘。
参数高效微调通常具有微调参数规模小、增量式微调参数、即插即用等特点，这种技术也统一成技术框架Delta Tuning[40]。一些围绕参数高效微调的开源工具也被研发，代表性包括OpenPrompt[55]、OpenDelta[56]等。由于不同任务的微调参数可以被重复利用，如AdapterHub[57]、Delta Center[40]等高效微调的仓库也被构建。随着语言大模型的兴起，高效微调吸引了越来越多的关注，以开发一种更轻量级的下游任务适配方法。特别地，LoRA[54]已广泛应用于各种开源语言大模型（如LLaMA）以实现参数高效微调。
2.3.3 语言大模型的提示学习
通过大规模文本数据预训练之后的语言大模型具备了作为通用任务求解器的潜在能力，但这些能力在执行一些特定任务时可能不会显式地展示出来。在大模型输入中设计合适的语言指令提示有助于激发这些能力，该技术称为模型提示技术。代表性的提示技术有指令提示和思维链提示：
指令提示（Instruction Prompt），也称为提示学习。OpenAI 在 GPT-3 [16]中首次提出上下文提示，并发现GPT-3 在少样本提示下能够达到人类水平，证明在低资源场景下非常有效，引起广泛关注。指令提示核心思想是避免强制语言大模型适应下游任务，而是通过提供“提示（Prompt）”来给数据嵌入额外的上下文以重新组织下游任务，使之看起来更像是在语言大模型预训练过程中解决的问题[10]。指令提示有三种形式：（1）少样本提示，（2）零样本提示，（3）上下文学习。
情境学习（In-Context Learning, ICL），也称情境学习，是指将自然语言问题作为语言大模型的输入，以答案作为输出。情境学习可以看作是一种特殊形式的少样本提示，问题中隐含了目标任务和格式信息。它简化了问题表示和答案生成，并能灵活处理多种类型和复杂度的问题。挑战在于确保问题质量和评估答案的正确性。
思维链（Chain-of-Thought，CoT）是一种提示技术，通过多步推理允许产生可验证的输出，提高黑盒模型的可解释性。它被广泛用于激发语言大模型的多步推理能力，鼓励模型生成解决问题的中间推理链。在思维链提示中，中间自然语言推理步骤的例子取代了少样本提示中的〈输入，输出〉对，形成了〈输入，思维链，输出〉三元组结构。思维链被认为是语言大模型的“涌现能力”，通常只有模型参数规模增大到一定程度后，才具有采用思维链能力。激活这种能力的方法是在提示中给出逐步的推理演示作为推理的条件，每个演示都包含了一个问题和一个通向最终答案的推理链。
知识运用和推理能力是衡量语言大模型智能水平的重要因素。有效的解决方法是在深度学习模型基础上融入各类型相关外部知识。根据大模型知识融合部位不同，知识融合方法从模型输入、神经架构、模型参数、输出等不同层面，大致分为以下4类：知识增广、知识支撑、知识约束。知识增广是从输入端增强模型，有两种主流的方法：一种方式是直接把知识加到输入，另一方法是设计特定模块来融合原输入和相关知识化的输入表示。知识支撑关注于对带有知识的模型本身的处理流程进行优化。知识约束则是利用知识构建额外的预测目标和约束函数，来增强模型。
文章内容如下：
型的原始目标函数。例如，远程监督学习利用知识图谱启发式标注语料作为新的目标，并广泛用于实体识别、关系抽取等系列NLP 任务。或者利用知识构建额外的预测目标，在原始语言建模之外构建了相应额外的预训练目标。
知识迁移：模型知识作为重要的知识来源，也可以直接用于下游任务，例如初始化模型参数。迁移学习和自监督学习都是知识迁移的重要研究方向。目前，知识迁移技术已被广泛应用于自然语言处理，以BERT 为首的各种预训练模型是现在知识迁移的主要方法。
语言大模型的工具学习：语言大模型具备理解、推理和决策能力，可与外部工具互动。在特定领域任务中，如金融领域的证券交易和市场预测，语言大模型通常需要结合外部工具获取信息和技能才能处理。整合外部工具与语言大模型可以发挥各自优势实现复杂任务的处理，其中外部工具可增强专业知识和可解释性，语言大模型提供语义理解和推理规划能力。
2021 年底，OpenAI 推出WebGPT，利用GPT-3 与网页浏览器和搜索引擎交互获取互联网信息在长文本问答上实现非常强的能力，展现了语言大模型利用工具解决复杂问题的巨大潜力。该工作引起了学术界和产业界的广泛关注，产生了许多面向不同任务或场景需求的大模型调用工具的方法，如Webshop，使用语言大模型替代人在购物平台上执行一系列操作、购买所需物品。
2023 年3月，OpenAI发布ChatGPT Plugins，实现ChatGPT 调用各种外部插件的功能，支持浏览器实时信息获取、代码解释器、PDF 阅读等能力，截至8月已支持480 个常用工具插件。Meta 将这种通过非参数的外部模块扩展语言大模型能力的方法，统一称为增广语言模型。
清华大学在现有大模型工具使用方法基础上，提出了工具学习框架，指在让模型能够理解和使用各种工具完成任务的学习过程。
目前可交互的通用工具按用户接口大致可分为三类：物理交互的工具（如机器人、传感器等）、基于图形用户界面的工具（如浏览器、Office 办公软件等）、基于编程接口的工具（如数据库、知识图谱）等。从学习目标的角度来看，现有工具学习方法主要可以分为两类：一类是工具增强学习（Tool-augmented Learning），利用各种工具的执行结果，增强基础模型性能。在这一范式中，工具执行结果被视为辅助生成高质量输出的外部资源；第二类是工具导向学习（Tool-oriented Learning），将学习过程重点从增强模型性能转向工具执行本身。这一类研究关注开发能够代替人类控制工具并进行序列决策的模型。
第3章 多模态大模型技术
不同于语言大模型只对文本进行处理，多模态大模型将文本、语音、图像、视频等多模态数据联合起来进行学习。多模态大模型融合了多种感知途径与表达形态，能够同时处理和理解来自不同感知通道（例如视觉、听觉、语言和触觉等）的信息，并以多模态的方式表达输出。
3.1 多模态大模型的技术体系
现有的多模态大模型主要有面向理解任务的、面向生成任务的、兼顾理解和生成的、知识增强的多模态大模型。
3.1.1 面向理解任务的多模态大模型
面向理解任务的多模态大模型，其核心结构通常是基于Transformer的编码器。按照模型结构的不同，面向理解任务的多模态大模型又可再分为单流和多流两种结构。单流结构是指不同模态的特征在拼接后由一个共享的Transformer网络进行处理；而多流结构中，不同模态则分别由Transformer网络进行编码处理，这些网络之间存在有一些特征上的交互融合机制。
多流结构的一个典型代表是图文理解模型ViLBERT，它采用了一种双流Transformer的结构，首先将文本和图像数据分别输入两个独立的Transformer编码器，接着使用互注意力Transformer（Co-Attention Transformer）层将文本和图像特征进行融合，最后所得到文本-图像特征可以被应用到视觉问答、图像描述生成等不同的多模态的任务中。多流结构的另一个代表是OpenAI公司的CLIP模型，它采用两个独立的编码网络对图像和文本进行特征抽取，并通过对比学习将两者的特征嵌入到共享的语义空间中。CLIP基于4亿图文对进行训练，可以从自然语言监督中有效地学习视觉概念，从而获得泛化性能极强的零样本（zero-shot）分类能力。
```
ALIGN[66]使用对比损失训练了一个简单的双编码器模型，利用包含超过10亿个噪声图像-文本对的数据集来扩展视觉和视觉语言表征学习。CLIP是个图文双流结构，VATT[67]则是针对视频-文本-音频数据的多流模型。VATT将每个模态线性投影为特征向量，然后将其分别送到Transformer编码器中，并在语义分层的不同粒度空间中通过对比学习来训练模型。
单流结构的一个典型代表是VL-BERT[68]，它将图像的描述文本和关键物体的区域特征拼接后作为BERT网络的输入，通过掩码掉部分文本输入和图像输入并预测所缺失的信息来进行模型训练。UNITER[69]则采用了一种多任务的多模态预训练方法，增加了单词与图像区域的匹配模块，来建立图像与文本的细粒度关联。
在视频领域，单流结构的代表性方法有VideoBERT[70]和ActBERT[71]，VideoBERT是一个视频-语言模型，融合了文本和视频作为BERT网络的输入；ActBERT采用了一种全局-局部关系的建模方法，输入包括文本和视频的全局信息，还利用了视频帧中的局部信息来加强对于视频内容的理解。
现有的面向理解任务的多模态大模型大多以上面两类结构为基础，也有不少方法在预训练任务上进行研究，引入更多的预训练任务或设计统一的架构去训练所有的任务等。
```
型方法Florence[72]，它着重于如何使模型适应各种下游任务，并设计了一个由多模态大模型和适应模型组成的工作流。具体对于任务适应，该模型使用动态头部适配器将学习到的视觉特征表示从场景扩展到对象，采用 CoSwin 适配器来学习视频表示，并使用METER 适配器将模型应用到依赖细粒度视觉-语言表示的视觉语言任务。
面向生成任务的多模态大模型能够实现文本、图片、视频、音频、3D、分子结构等多种模态内容的生成应用。目前常用的方法主要是基于序列生成模型和扩散模型（diffusion models）。
在序列生成模型中，DALL-E[73]是个典型代表。它是由OpenAI发布的一个基于4 亿图文对训练的图像生成模型，通过采用VQVAE[74]图像离散自编码器和GPT 组合的结构，在以文生图任务上取得了突破性的生成质量和泛化能力，被称作图像版GPT。另一典型的图像生成模型是北京智源研究院所的CogView 模型[75]，它具有与DALL-E 类似的结构，但是面向中文环境的文本到图像生成，并进一步探索了多模态生成模型在下游任务上精调后的泛化能力。CogView 在基于文本控制的样式学习、服装设计和图像超分等任务上均取得出色的效果。
在文本生成方向上，采用序列生成模型是最主流的方案，例如，典型方法GIT[76]是一个视觉到文本的多模态大模型，统一了图像/视频的描述和问答等视觉语言任务，其包含有一个图像编码器和一个文本解码器，其文本解码器在视觉编码的基础上，以自回归的方式来生成文本。
扩散模型的工作原理是通过连续添加高斯噪声来破坏训练数据，然后反转这个噪声过程来学习恢复数据。扩散模型的一种代表性方法是LDM，它先压缩图像像素信息获取图像对应的隐特征表达，再利用扩散模型来建模图像隐特征分布。Stable Diffusion是LDM的拓展，应用于开放领域的文本至图像生成，是当前开源模型的代表。闭源的扩散模型中，OpenAI的DALL-E2和谷歌的Imagen是代表性方法。DALL-E2通过训练扩散解码器和映射模型，将CLIP模型的文本特征映射到图像特征空间，提升生成图像与文本的匹配程度。Imagen则使用通用语言大模型T5直接编码文本信息，通过扩散模型生成图像，并逐步提高分辨率，发现基于T5模型提取的文本特征生成的图像细节准确度更高。
为了让模型同时具备理解与生成能力，从而在更广泛的下游任务上应用，可以联合Transformer编码器与解码器，设计兼顾理解与生成任务的多模态大模型。一个典型方法是蒙特利尔大学的VL-T5模型，它将多个多模态任务统一为文本生成任务。具体地，该模型由Transformer编码器和自回归的解码器组成，主要创新点在于针对训练任务与数据的不同采用不同的输入文本与输出文本的构造。
造方式，这种将模型结构和目标任务统一的方法可以充分利用不同任务的数据来训练模型，提高模型的泛化性。这类方法的另一个典型模型Unified VLP[81]，它的主要特点是编码器和解码器共享同一个Transformer网络。该方法通过设置注意力掩码来控制网络为编码器或解码器。具体地，当使用编码器时，注意力掩码为双向掩码，任一位置都可建模前后两个方向的依赖关系；当使用解码器功能时，注意力掩码设置为单向，每一位置只能建模前文的依赖关系。这种编解码共享的方式能够减少参数量，使网络更加简洁。
此外，还可以将语言大模型的文本生成能力与各类模态编码器的多模态感知能力相结合，以此构建的多模态大模型也能够兼顾理解和生成能力。这类方法以语言大模型为主导来实现多模态的对齐、融合和交互。这是由于文本有高效的表达效率、能够通过语义描述的方式与其余所有模态建立直接的联系，另外，语言大模型在预训练过程中学习到了非常多的世界知识，有潜在理解多模态信息的能力。这类模型在结构方面常由单模态编码器、连接器与语言大模型三部分组成，其中单模态编码器和语言大模型的参数可以冻结以减少计算量、提高训练效率；连接器常见的有简单的线性映射层，或者特殊设计的网络模块如BLIP-2[82]中的Q-former结构等。这类模型通常涉及到两个阶段的训练过程。在第一阶段，训练各个模态到语言大模型的语义对齐，通常利用大规模弱关联的跨模态数据（如图像-文本、视频-文本、音频-文本数据等），基于条件文本生成任务进行。
在第二阶段进行多模态指令微调以提升零样本多模态能力，此阶段的核心是构造面向多模态任务的指令微调数据，目前常见的多模态指令微调数据类型有多模态对话、多模态详细描述与多模态推理问答等。
大模型不仅对大规模数据有着卓越的拟合能力，还能够学习到隐式的知识。为了促进更有意义的理解和预测，还需要寻找将隐式知识与显式知识（例如来自知识图谱）联系起来的方法。因此，将知识图谱、场景图、外部知识库等结构化的知识信息注入大模型中，将增强多模态大模型的知识利用能力。例如，在场景图知识的利用上，一个典型方法是百度的ERNIE-ViL模型，它在视觉-语言模型中引入了由文本解析而来的场景图信息，在预训练过程中通过将场景图中的知识实体和关系进行掩码后要求模型预测所掩码位置的知识信息，以此作为更细的多模态预训练任务，这能够使得模型更能精准把握图像和文本之间细粒度的对齐信息。在知识图谱的利用上，典型方法有KRISP，它结合了隐含知识和明确知识的学习，即从无监督语料和有监督的训练数据中学到隐含的知识。
据知识图谱中学习明确的符号化知识，这样既可以进行隐式的知识推理，又可以获取符号化的知识表示。
多模态大模型的关键技术主要包括预训练数据收集、基础模型构建、自监督学习与模型优化训练、下游任务微调。
3.2.1 多模态大模型的网络结构设计
网络架构在多模态预训练中扮演着关键角色，需要精心设计以适应和理解来自不同源的复杂特征。例如，在处理图像和文本模态时，通常会采用Transformer或卷积神经网络（CNN）来捕捉视觉和语言之间的复杂关系；而对于事件流，脉冲神经网络可能更为适合，因为它们能有效地模拟信息的时序动态。
随着模型规模的增加，大型多模态大模型展示出强大的记忆能力和性能增益。然而，模型复杂度的增加也不可避免地引入了计算效率的挑战，并可能最终遇到性能瓶颈。因此，对于更高效的网络模型结构的设计和探索，比如改进或甚至替代Transformer，成为了重要的研究方向。
其次，得益于语言大模型涌现出的知识与逻辑推理能力，近期有一系列多模态大模型开始以语言大模型为核心进行构建。其中一个代表性方法是DeepMind的Flamingo视觉语言模型，该模型能够将图像、视频和文本作为提示并输出相关语言回复。它将视觉编码器与语言大模型的参数冻结并通过可学习的融合模块联系起来。
20多亿对图片-文本、270万对视频-文本，以及430万图文混排的网页数据进行视觉-语言联合训练；Flamingo具有少样本的多模态序列推理能力，无需额外训练即可完成视觉语义描述、视觉问答等多种任务。另一个代表性模型KOSMOS-1，它将基于Transformer的语言模型作为通用接口，并与视觉感知模块对接，使模型“能看”和“会说”；该模型具有16亿参数量，在大规模多模态语料库上训练，具有遵循指令（即零样本学习）以及在上下文中学习（即少样本学习）能力，能够原生处理视觉对话、视觉问答、图像描述生成、光学字符识别等任务。近期还有一系列模型尝试将图像、视频等感知模块与LLaMA等开源的语言大模型对接，实现类似GPT-4的多模态理解能力。ChatBridge是其中一个典型模型，它使用多个并行的感知模块处理图片、音频、视频特征，通过少量预训练参数将这些特征投影至语言大模型的语义空间，使模型具备灵活感知、理解混合模态信息的能力。
对于多模态预训练，设计与下游任务更高兼容性的网络结构模型尤为重要。可以通过引入编码器-解码器结构将多模态理解和生成任务统一到一个框架下，更好地支持各种多模态任务。这主要涉及到跨模态的注意机制、模态间的对齐和翻译，以及更复杂的特征集成策略。
以视觉-语言数据的联合学习为例，多模态大模型常用的自监督学习任务有以下几种类型：
1）掩码语言建模（Masked Language Modeling，MLM）：输入文本序列中的某些单词或标记会被替换为特殊的掩码标记，然后预训练模型被要求根据可见的多模态上下文来预测这些被遮蔽的单词或标记。
能够在大规模文本数据上获取深层次的语言理解，从而更好地执行下游自然语言处理任务，如文本分类、命名实体识别、句子相似性计算等。掩码图像建模（Masked Image Modeling，MIM）：输入图像中的部分区域会被隐藏或被替换为特殊的掩码标记，然后预训练模型被要求在仅看到其余图像内容与文本等其他模态信息的情况下，预测或还原被遮蔽的图像区域。多模态大模型通常使用这种训练方式促使模型学习图像的视觉特征、多模态上下文信息和语义关系，以更好地理解图像内容。
图像-文本匹配（Image-Text Matching，ITM）旨在实现图像与文本的全局对齐。通常给定图文对作为正样本，随机配对作为负样本对，通过二分类方法实现图像和文本的匹配，建立图像和文本之间的语义关联。
图像-文本对比学习(Image-Text Contrastive Learning, ITC)使用对比学习的方法，将图像和文本的相同样本对的向量表示拉近，不同样本对的向量表示推远，从而增强图像和文本之间的语义关联性，为多模态任务提供更好的表示能力。
多模态大模型的最终目标是适配并提升特定下游任务上的性能表现，因此需要通过微调适配将预训练大模型的能力迁移到特定任务。
目前，多模态大模型的微调适配方式主要有三种：
1）面向特定任务的模型微调适配：这种方式下，多模态大模型的权重被作为初始参数，在任务特定数据上进行有监督的微调。通过这种微调，模型能够学习到针对具体任务的细粒度特征和表示，以适应特定任务的要求。
2）联合提示学习的模型微调适配：设计适合上游预训练任务的模板，挖掘上游预训练模型的潜力，使得模型在不依赖标注数据的情况下也能较好地完成下游任务。提示学习允许在不同类型的任务上重复使用预训练模型，只需简单修改提示模板即可适应特定任务，从而节省训练时间和计算资源。
3）基于适配器网络的模型微调适配：每个任务拥有独立的适配器层，使得模型可以在不同任务之间共享通用预训练模型的表示，同时进行个性化调整。适配器层通常由较少的参数组成，比在整个模型上进行微调更为高效。在训练过程中，预训练模型的参数保持固定，仅更新适配器层的参数。
现有的预训练大型方法通过特征微调或提示学习用于下游任务，也需要更多研究考虑为多模态大型模型开发增量学习算法。未来，如何将新模态引入已预先训练好的多模态模型中具有实际意义，因为新的传感器（模态）将在未来的某个不确定时间出现，设计的多模态大型模型应具备足够的灵活性以应对这种情况。
随着大模型技术的快速发展，大模型的生态体系也在快速构建。典型的大模型平台如ChatGPT、文心一言、讯飞星火等提供多种形式的开放服务，并通过开放插件机制、Function Call等实现大模型外部工具、服务的调用，加速应用生态的发展。开源大模型已经成为生态体系中的关键组成部分，通过大模型的开源共建，凝聚了来自企业、高校、科研院所等领域高水平开发者的力量，加速大模型的科研创新和产品迭代。伴随着大模型的开源开放，深度学习开源框架和工具更加注重分布式训练和推理能力，并加速与AI芯片开展适配和联合优化。大模型的训练数据作为生态中另一关键组成部分，相关数据集和配套工具也在加速汇聚和优化，愈发得到广泛重视。
4.1 典型大模型平台 
（1）GPT 系列 
OpenAI的GPT系列模型是自然语言处理领域的重大突破，其中ChatGPT和GPT-4是两个代表性模型。ChatGPT专注于对各种文本指令做出回应，它可以执行各种任务，包括代码编写、数学问题求解、写作建议等。GPT-4在推理方面的能力比ChatGPT更强，同时也减少了幻象的产生，能够更准确地理解和回应复杂的问题，从而提供更高质量的答案。现在，ChatGPT Plus用户可以使用各种插件来增强模型以满足自己的需求，这极大地扩展了模型的用途和适用领域。 
（2）Claude 系列
Claude系列模型是由Anthropic开发的闭源语言大模型，包含Claude和Claude-Instant两种模型。最早的Claude于2023年3月15日发布，并在7月11日更新至Claude-2。该系列模型通过无监督预训练、基于人类反馈的强化学习和Constitutional AI技术进行训练，以改进模型的有用性、诚实性和无害性。Claude最高支持100K词元的上下文，而Claude-2拓展到了200K词元。与Claude 1.3相比，Claude 2有更强的综合能力，能生成更长的相应。
PaLM系列语言大模型由Google开发，其初始版本于2022年4月发布，2023年3月公开了API。PaLM基于Google提出的Pathways机器学习系统，训练数据总量达780B个字符，涵盖多种语料形式。PaLM有8B、62B、540B三个不同参数量的版本。Google还开发了Med-PaLM、PaLM-E等改进版本。2023年5月，Google发布了PaLM 2。
Bard是Google开发的对话模型，发布于2023年2月6日，基座模型是Google此前开发的LaMDA。Google为Bard进行了持续的升级，包括添加数学与逻辑能力、代码能力，支持更多语言等。
年5月，Google发布了基于新一代语言大模型PaLM 2的Bard。
文心一言是基于百度文心大模型的知识增强语言大模型，于2023年3月在国内率先开启邀测。文心一言的基础模型文心大模型于2019年发布。8月31日，文心一言率先向全社会全面开放，提供APP、网页版、API接口等多种形式的开放服务。文心一言一方面采用有监督精调、人类反馈的强化学习、提示等技术，还具备知识增强、检索增强和对话增强等关键技术。当前，以文心一言为代表的大模型已经逐步赶超国外最优水平。文心一言基于飞桨深度学习框架进行训练，算法与框架的协同优化后效果和效率都得到提升，模型训练速度达到优化前的3倍，推理速度达到优化前的30多倍。文心一言还建设了插件机制，通过外部工具、服务的调用，拓展大模型的能力的边界。
讯飞星火认知大模型是科大讯飞于2023年5月6日发布的语言大模型，提供了基于自然语言处理的多元能力，支持多种自然语言处理任务。同时联合中科院人工智能产学研创新联盟和长三角人工智能产业链联盟在业内提出了覆盖7大类481项任务的《通用人工智能评测体系》；6月9日星火大模型升级到V1.5版，实现了开放式知识问答、多轮对话、逻辑和数学能力的提升；8月15日星火大模型升级到V2.0版，对于代码和多模态能力进行了提升。同时，讯飞和华为还联合重磅发布了国内首款支持大模型训练私有化的全国产化产品“星火一体机”，可支持企业快速实现讯飞星火大模型的私有化部署、场景赋能和专属大模型训练优化。
腾讯混元大模型是腾讯于2023年9月7日发布的千亿参数量语言大模型，具有多轮对话、内容创作、逻辑推理、知识增强能力，训练数据截止于2023年7月。为了降低幻觉问题，混元大模型在预训。
混元大模型针对位置编码进行了优化，并结合指令跟随能力解决长难任务。此外，混元大模型还具备了问题分解和分布推理能力，从而解决逻辑推理问题。
通义千问由阿里巴巴基于“通义”大模型研发，于2023年4月正式发布。它能够以自然语言方式响应人类的各种指令，拥有强大的能力，如回答问题、创作文字、编写代码、提供各类语言的翻译服务、文本润色、文本摘要以及角色扮演对话等。借助于阿里云丰富的算力资源和平台服务，通义千问能够实现快速迭代和创新功能。
4.2 典型开源大模型
4.2.1 典型开源语言大模型
LLaMA 系列 Meta LLAMA，LLAMA2 7B，13B，65B
Falcon 系列 TII Falcon 1.3B，7.5B，40B，180B
Pythia 系列 EleutherAI Pythia 70M，160M，410M，1B，1.4B，2.8B，6.9B，12B
T5 系列 Google T5，mT5，FLAN-T5 60M，220M，770M，3B，11B
BLOOM 系列 BigScience BLOOM，BLOOM-Z 560M，1.1B，1.7B，3B，7.1B，176B
GPT-Neo EleutherAI GPT-Neo 125M，350M，1.3B，2.7B
OPT 系列 Meta OPT，OPT-IML 125M，350M，1.3B，2.7B，6.7B，13B，30B，66B，175B
MPT 系列 MosaicML MPT-Chat 7B，30B
MPT-Instruct
文心系列 百度 ERNIE 1.0，ERNIE 2.0，ERNIE 3.0 18M，23M，75M，100M，118M，280M
LLaMA系列模型[30]是一组参数规模从7B到65B的基础语言模型，它们在数万亿个字符上训练，展示了如何仅使用公开可用的数据集来训练最先进的模型，而不需要依赖专有或不可访问的数据集。这些数据集包括Common Crawl、Wikipedia、OpenWebText2、RealNews、Books等。LLaMA模型使用了大规模的数据过滤和清洗技术，以提高数据质量和多样性，减少噪声和偏见。LLaMA模型还使用了高效的数据并行和流水线并行技术，以加速模型的训练和扩展。特别地，LLaMA 13B在CommonsenseQA等9个基准测试中超过了GPT-3 (175B)，而LLaMA 65B与最优秀的模型Chinchilla-70B和PaLM-540B相媲美。LLaMA通过使用更少的字符来达到最佳性能，从而在各种推理预算下具有优势。与GPT系列相同，LLaMA模型采用了decoder-only架构，但同时结合了一些前人工作的改进，例如：Pre-normalization，为了提高训练稳定性，LLaMA对每个Transformer子层的输入进行了RMSNorm归一化，这种归一化方法可以避免梯度爆炸和消失的问题，提高模型的收敛速度和性能；SwiGLU激活函数。
将ReLU非线性替换为SwiGLU激活函数，增加网络的表达能力和非线性，同时减少参数量和计算量；RoPE位置编码，模型的输入不再使用位置编码，而是在网络的每一层添加了位置编码，RoPE位置编码可以有效地捕捉输入序列中的相对位置信息，并且具有更好的泛化能力。这些改进使得LLaMA模型在自然语言理解、生成、对话等任务上都取得了较好的结果。
Falcon系列模型是由位于阿布扎比的技术创新研究院(Technology Innovation Institute, TII)创建的生成式语言大模型，基于Apache 2.0许可发布。Falcon大模型家族目前主要包含三个基础模型: Falcon-7B，Falcon-40B，以及Falcon-180B。三个模型都是在RefinedWe数据集上训练的，该数据集经历了广泛的过滤和去重过程，以确保高质量的训练数据。同时，三个模型均可用于研究和商业用途。
Pythia系列模型是由非营利性人工智能实验室EleutherAI开发的一系列生成式语言大模型。该系列有16个不同参数量的模型。
EleutherAI 使用相同的架构训练了2套Pythia版本。每一套包含8个模型，涵盖8种不同的模型尺寸。一套是直接在Pile上训练的，另一套则在经过MinHashLSH近重复处理后的Pile上进行训练，阈值设置为0.87。经过去重处理后Pile大约包含207B个字符，而原始Pile包含300B个字符。由于Pythia系列模型在相同架构基础上涵盖多个不同尺寸，Pythia很适合被用来研究诸如性别偏见、记忆能力和少样本学习等属性如何受到精确训练数据处理和模型规模的影响。目前，Pythia系列的模型可以在开源模型网站Hugging Face上直接获取，也可以通过Github的官方页面获取。
T5[42]模型是由Google Brain团队在2019年提出的一种基于Transformer结构的序列到序列（Seq2Seq）模型，其主要特点是将多种NLP任务（如翻译、摘要、问答等）转化为一个统一的框架下进行训练，使用文本到文本的统一模型范式，保证了模型的灵活性。T5模型使用了混合精度训练和自适应优化器来加速训练过程，并且使用了数据过滤和动态批处理来提高数据效率。T5模型在多个NLP任务上都取得了较好的效果，证明了其优秀的泛化能力和迁移能力。T5模型在预训练阶段使用了C4数据集，这是一个包含了超过750GB的英文网页文本数据的大规模语料库。T5模型还探索了不同规模的模型架构和参数量，从小到大分别有small、base、large、XL、XXL和XXXL六种规模。
BigScience在2022年提出了BLOOM系列模型[92]。
BLOOM模型有1760亿参数量，是基于Transformer解码器架构的语言大模型，并在46种自然语言和13种编程语言上进行预训练。为了提升BLOOM模型的多语能力，研究者采用了渐进的方式来选择语料库中包含的语言。BLOOM对原始的Transformer架构提出了许多更改，如采用ALiBi技术计算注意力分数，归一化层在嵌入层之后立即进行，显著改善训练稳定性。BLOOM最终确定的词表尺寸为25万个字符，以支持多种语言。BLOOMZ与BLOOM拥有相同的模型架构与超参数，在包含130亿字符的文本上进行微调，模型的性能通过独立的验证集来选择最优。此外，对于13亿参数量和71亿参数量的版本，研究者使用了SGPT Bi-Encoder方案进行对比微调。
GPT-Neo系列模型是由EleutherAI开发的预训练语言大模型，基于OpenAI的GPT系列语言模型的架构，但采用了分散、社区驱动的方法进行训练。GPT-Neo模型在发布时因其较大参数规模和在各种自然语言处理任务中的出色表现而备受关注。其最大版本GPT-Neo 2.7B有27亿个参数，在多样化的互联网文本数据上进行训练，包括书籍、文章和网页，并在广泛的自然语言处理任务上表现良好。
GPT-Neo 项目的独特之处在于强调开源开发和社区参与。EleutherAI 公开了该模型的训练权重，使其他研究人员和开发人员能够使用和构建该模型，并开发出许多相关的应用和GPT-Neo 模型的扩展，包括对特定任务的微调和修改，以提高其在某些特定类型的数据上的效率或性能。
OPT 模型是由Meta AI 发布的一款decoder-only 模型，与GPT-3 相媲美。Meta AI 发布了OPT 模型，其参数规模从125M 到175B 不等，并开源了相关的实验代码。团队还公开了详细的训练日志，深入解释了他们的决策背后的原因和动机。在构建训练语料方面，OPT 使用了多个高质量语料库，这些语料库都经过了严格的收集和过滤，以确保数据的质量和可用性。
MPT 系列（MosaicML Pretrained Transformer）模型是由MosaicML 研发的开源可商用模型。MPT-7B 在2023 年5 月发布，包括MPT-7B-Instruct、MPT-7B-Chat 以及MPT-7B-StoryWriter-65k+三个版本。2023年6月，MPT-30B 发布，拥有比MPT-7B 更强大的性能，超过了原始的GPT-3，并且也有两个经过微调的变体。
MPT-30B-Instruct和MPT-30B-Chat在单轮指令跟随和多轮对话方面表现出色。MPT-30B在训练时使用8,000字符长度的上下文窗口，并通过ALiBi支持更长上下文以及通过FlashAttention实现高效的推理和训练性能。得益于预训练数据混合比例的控制，MPT-30B系列还具有强大的编程能力。
2019年，百度将大规模知识与海量数据融合学习的方法引入超大规模模型中，引入丰富语言知识与世界知识，突破多源异构数据难以统一表示与学习的瓶颈，显著提升了模型效果和学习效率，并在国内开源首个中文预训练大模型ERNIE。ERNIE自发布以来在语言理解、文本生成、跨模态语义理解等领域取得多项技术突破，在权威公开数据集上取得世界最好效果总计90余项，在国际权威语义评测GLUE、SuperGlue等评测上，取得世界冠军20余项。系列模型在金融、通信、企业服务、互联网等行业取得广泛应用，极大促进该领域在国内的研究和产业发展。ERNIE 3.0大模型最高参数量达到1000亿，首次在百亿级预训练模型中引入大规模知识图谱，提出了海量无监督文本与大规模知识图谱的平行预训练方法，促进了结构化知识和无结构文本之间的信息共享，大幅提升了模型对于知识的记忆和推理能力。
GLM系列模型是清华大学和智谱AI等合作研发的开源语言大模型。GLM采用了自回归填空作为预训练任务，并且使用多任务预训练的方式提升模型生成长文本的能力和序列到序列任务的能力。为了能够更好地进行预训练，GLM采用了二维位置编码，第一维表示当前位置的原文本中的位置信息，第二维表示对应的掩码的位置信息。此外，为了能够尽量减少推理和训练所占用的显存，GLM-130B可以使用INT4进行量化并且不会明显影响模型效果。通过优化，GLM-130B可以在4张RTX 3090 Ti（24G）显卡或8张RTX 2080 Ti上运行。
ChatGLM 是基于GLM结构开发的具有62亿参数量的语言大模型，支持2048的上下文长度。其使用了包含1万亿字符的中英文语料进行训练，能够支持中文和英文两种语言的 task。通过监督微调、反馈自助、人类反馈强化学习等多种训练技术，ChatGLM 拥有强大的生成能力，能够生成更符合人类偏好的内容。与GLM相似，通过INT4量化和P-Tuning v2等高效微调的算法，ChatGLM能够在7G显存的条件下进行微调。在ChatGLM的基础上，ChatGLM 2使用了包含1.4万亿字符的中英预料进行预训练，并使用人类偏好的数据对模型进行对齐训练，拥有比前一版本更加强大的能力，在多个任务上取得提升。通过FlashAttention技术，ChatGLM 2能够处理更长的长下文，支持的长下文长度达到了3.2万字符。此外，通过Multi-Query Attention技术，ChatGLM 2能够进一步地提升推理速度，减小对显卡的显存占用。
Baichuan是由百川智能开发的开源可商用的语言大模型，在权威的中文和英文benchmark上均取得同尺寸最好的效果，其基于Transformer解码器架构。Baichuan-7B是在大约1.2万亿字符上训练的70亿参数模型，支持中英双语，最大4096的上下文窗口长度。Baichuan-13B在Baichuan-7B的基础上进一步扩大参数量到130亿，并且在高质量的语料上训练了1.4万亿字符，超过LLaMA-13B 40%，是当前开源13B尺寸下训练数据量最多的模型。其支持中英双语，使用ALiBi位置编码，最大4096的上下文窗口长度，使用rotary-embedding，是现阶段被大多数模型采用的位置编码方案，具有很好的外推性。百川同时开源了预训练和对齐模型，预训练模型是面向开发者的“基座”，而对齐模型则面向广大需要对话功能的普通用户。除了原始权重，为实现更高效的推理，百川开源了INT8和INT4的量化版本，相对非量化版本在几乎没有效果损失的情况下大大降低。
了部署的机器资源需求。Baichuan2-7B 和 Baichuan2-13B，均基于2.6万亿高质量多语言数据进行训练，在保留了上一代开源模型良好的生成与创作能力，流畅的多轮对话能力以及部署门槛较低等众多特性的基础上，两个模型在数学、代码、安全、逻辑推理、语义理解等能力有显著提升。
CPM 系列模型由北京智源人工智能研究院和清华大学的合作研发，目前包括了CPM-1、CPM-2，CPM-3 和CPM-Bee 典型模型。CPM-1，作为首款中文大规模预训练语言模型，拥有26亿参数。其预训练任务采用了经典的自回归语言模型，以100GB 数据为基础，包括大量丰富多样的中文语料，在多个公开的中文数据集上的实验表明，CPM-1 在对话、文本生成等各类下游任务中，无论是少样本学习还是零样本学习，都表现出卓越的性能。CPM-2模型采用“编码器-解码器”框架，通过词表优化、知识继承、混合专家化等技术，显著缓解了大规模预训练模型训练的计算开销对应用的使用限制。CPM-3 是基于BMTrain高效训练框架实现，在预训练阶段采用多样化的任务设计和提示模板预训练技术，在零样本和少样本场景中表现出色。CPM-Bee 是一个完全开源、允许商用的百亿参数中英文基座模型。它采用Transformer自回归架构，通过对预训练预料进行严格后处理提升数据质量，最终在万亿级高质量数据上完成预训练，进一步强化了模型的基础能力。
盘古系列鹏程·盘古α由以鹏城实验室为首的技术团队联合协作开发的，他们首次利用“鹏城云脑Ⅱ”和国产MindSpore框架，采用自动混合并行模式，在2048卡算力集群上进行大规模分布式训练，训练出业界首个以中文为核心2000亿参数的预训练生成语言模型。
程.盘古α具备丰富的应用场景，如知识问答、知识检索、知识推理、阅读理解等，并且拥有很强的小样本学习能力。鹏程.盘古α收集了近80TB的原始数据，包括开源数据集、common crawl网页数据、电子书等，搭建了面向大型语料库预处理的分布式集群。通过数据清洗过滤、去重、质量评估等处理流程，构建了一个约1.1TB的高质量中文语料数据集。研究对比了智源研究院发布的首个26亿参数的中文预训练语言模型「悟道·文源」CPM，通过在1.1TB数据中策略抽样了100GB等量数据集训练了2.6B参数规模的「鹏程.盘古α」模型，并在已收集的16个下游任务上进行了对比。实验结果表明，鹏程.盘古α-2.6B比CPM-2.6B模型具有更强的语言学习能力，特别是在生成任务和小样本学习方面。实验还对比了鹏程.盘古α-13B和鹏程.盘古α-2.6B模型的性能。在所有的生成任务和大部分的PPL任务上，13B的模型性能优于2.6B，说明鹏程.盘古α-13B模型具有较强的小样本学习能力。
KOSMOS-2是微软亚洲研究院在KOSMOS-1模型基础上开发的多模态大模型。KOSMOS-1在大规模多模态数据集上进行了重头训练，具有类似GPT-4的多模态能力，可以感知一般的感官模态，在上下文中学习并能够遵循语音指示。KOSMOS-2采用了相同的模型架构和训练目标，并新增了对图像局部区域的理解能力。
OpenFlamingo模型是DeepMind Flamingo模型的开源复现版，通过交叉注意力将预训练的视觉编码器和一个语言大模型结合在一起，在大型多模态数据集上进行训练，能够以交错的图像/文本为输入进行文本生成。
BLIP-2通过轻量级的查询转换器弥补了模态之间的差距，分两个阶段进行预训练，在视觉语言任务上实现了最先进的性能。
InstructBLIP设计了一种视觉语言指令微调方法，基于预训练的BLIP-2模型，对视觉语言指令进行微调，复用了BLIP-2的结构，并采用了指令感知的视觉。
觉特征提取过程，指令不仅会指导语言大模型生成文本，同时也会指导图像编码器提取不同的视觉特征。这样的好处在于对于同一张图片，根据不同的指令，可以得到基于指令偏好更强的视觉特征，同时对于两个不一样的图片，基于指令内嵌的通用知识，可以使得模型有更好的知识迁移效果。
MiniGPT-4使用语言大模型来增强视觉语言理解，将语言能力与图像能力结合。其利用视觉编码器和语言大模型Vicuna进行结合训练。具体地，MiniGPT-4使用一个投影层来将来自BLIP-2的冻结视觉编码器与冻结的Vicuna语言大模型（基于LLaMA指令微调得到）对齐。并通过两个阶段来训练MiniGPT-4。第一个预训练阶段使用大约500万个图像-文本对进行视觉-语言对齐训练。第二个微调阶段进行多模态指令微调以提高其生成可靠性和整体可用性。
LLaMA-Adapter V2是一种参数高效的视觉指令模型。具体地，首先通过解锁更多可学习参数（例如范数、偏差和比例）来增强LlaMA Adapter，这些参数将指令遵循能力分布到整个LLaMA模型中。其次，采用了一种早期融合策略，将视觉标记提供给早期的语言大模型，有助于更好地整合视觉知识。然后，通过优化不相交的可学习参数组，引入了图像-文本对和指令跟随数据的联合训练范式。该策略有效地缓解了图文对齐和指令跟随这两个任务之间的干扰，仅用小规模的图文和指令数据集就实现了强多模态推理。
ImageBind是Meta发布的模型，它的目标是利用图像为中心。
绑定学习一个嵌入空间，将文本、图像/视频、音频、深度（3D）、热（红外辐射）和惯性测量单元（IMU）六个模态的数据都投影到这个嵌入空间中。进而，在这个空间中可以实现跨模态检索和匹配等任务。ChatBridge是一个新型的多模态对话模型，利用语言的表达能力作为桥梁，以连接各种模式之间的差异，可支持文本、图像、视频、音频几个模态任意组合的模型输入与输出信息。VisualGLM-6B是由语言模型ChatGLM-6B与图像模型BLIP2-Qformer结合而得到的一个多模态大模型，其能够整合视觉和语言信息，可以用来理解图片，解析图片内容。VisCPM是一个多模态大模型系列，其中的VisCPM-Chat模型支持中英双语的多模态对话能力，而VisCPM-Paint模型支持文到图生成能力。
通过英文多模态数据预训练，泛化实现中文多模态能力。
阿里达摩院的mPLUG-Owl大模型可以支持多种数据模态，包括图像、文本、音频等。它采用了预训练和微调的方法，通过使用大规模的预训练数据和对特定任务微调的数据，可以快速高效地完成各种多模态任务。与传统的多模态模型相比，mPLUG-Owl有更高的准确率和更快的运行速度。此外，它还具有高度的灵活性和可扩展性，可以根据实际需要进行快速部署和优化。
Qwen-VL是支持中英文等多种语言的视觉语言模型。Qwen-VL以通义千问70亿参数模型Qwen-7B为基座语言模型，在模型架构上引入视觉编码器，使得模型支持视觉信号输入，并通过设计训练过程，让模型具备对视觉信号的细粒度感知和理解能力。除了具备基本的图文识别、描述、问答及对话能力之外，Qwen-VL还具备视觉定位、图像中文字理解等能力。
PyTorch自身提供了几种加速分布数据并行的技术，包括分桶梯度、通信和计算的重叠以及在梯度累积阶段跳过梯度同步。PyTorch分布式数据并行可以用256个GPU达到接近线性的可扩展性程度。在DP的基础上，原生支持DDP，每个节点都有自己的本地模型副本和本地优化器，支持多机多卡的分布式训练。一般来说，DDP都显著快于DP，能达到略低于卡数的加速比，但要求每块GPU卡都能装载完整输入维度的参数集合。在1.11版本后，PyTorch开始支持FSDP技术，可以更加高效的将部分使用完毕的参数移至内存中，显著减小了显存的峰值占用，更加吻合大模型的特性。
TensorFlow是一款由Google Brain团队开发的开放源代码机器学习框架，被广泛应用于各种深度学习领域。它可以处理多种数据类型，包括图像、语音和文本等，具备高度的灵活性和可扩展性。TensorFlow使用数据流图计算模型来建立机器学习模型，用户可以通过定义操作和变量在数据流图上构建自己的神经网络模型。此外，TensorFlow还提供了众多优化器、损失函数和数据处理工具，以便用户轻松进行模型训练和优化。TensorFlow在多个领域有广泛的应用，包括自然语言处理、图像识别和语音识别等。
PaddlePaddle（飞桨）是我国较早开源开放、自主研发、功能完备的产业级深度学习框架。飞桨不仅在业内最早支持了万亿级稀疏参数模型的训练能力，而且近期又创新性地提出了4D混合并行策略，以训练千亿级稠密参数模型，可以说分布式训练是飞桨最具特色的技术之一。飞桨的分布式训练技术在对外提供之前就已经在百度内部广泛应用。
MindSpore是一款适用于端边云全场景的开源深度学习训练/推理框架。MindSpore能很好匹配昇腾处理器算力，在运行高效和部署灵活上具有很好的能力。MindSpore还具有无缝切换静态图动态图、全场景覆盖、新AI编程范式等特点。
Jittor是一个基于即时编译和元算子的高性能深度学习框架。
Jittor 集成了算子编译器和调优器，可以为模型生成高性能的代码。Jittor 与 PyTorch 兼容，可以方便地将 PyTorch 程序迁移到 Jittor 框架上。Jittor 支持多种硬件平台，包括 CPU、GPU、TPU 等。Jittor 在框架层面也提供了许多优化功能，如算子融合、自动混合精度训练、内存优化等。
OneFlow 能够较好适用于多机多卡训练场景，是国内较早发布的并行计算框架。OneFlow 会把整个分布式集群逻辑抽象成为一个超级设备，用户可以从逻辑视角的角度使用超级设备。最新版本的 OneFlow 实现了同时对动态图和静态图的支持，而且动静图之间转换十分方便。此外，OneFlow 兼容了 PyTorch，支持数据 + 模型的混合并行方式，可提升并行计算性能。
Colossal-AI 提供了一系列并行组件，通过多维并行、大规模优化器、自适应任务调度、消除冗余内存等优化方式，提升并行训练效率，并解耦了系统优化与上层应用框架、下层硬件和编译器，易于扩展和使用。提升人工智能训练效率的同时最小化训练成本。夸父从大模型实际训练部署过程中的性价比角度出发，力求易用性，无需用户学习繁杂的分布式系统知识，也避免了复杂的代码修改。
Megatron-LM 是 NVIDIA 提出的一种基于 PyTorch 分布式训练大规模语言模型的架构，用于训练基于 Transformer 架构的巨型语言模型。针对 Transformer 进行了专门的优化，主要采用的是模型并行的方案。Megatron 设计就是为了支持超大的 Transformer 模型的训练的，因此它不仅支持传统分布式训练的。
数据并行，也支持模型并行，包括Tensor并行和Pipeline并行两种模型并行方式。同时提出了更加精细的pipeline结构与communication模式。通过多种并行方式的结合，可以让大模型的训练更快。将核心操作LayerNorm和Dropout安装输入维度进一步切分，使得这两个需要频繁运行的操作在不大幅增加通信开销的情况下实现了并行。
DeepSpeed：在2021年2月份，微软发布了一款名为DeepSpeed的超大规模模型训练工具，其中包含了一种新的显存优化技术——零冗余优化器(Zero Redundancy Optimizer, ZeRO）。该技术去除了在分布式数据并行训练过程中存储的大量冗余信息，从而极大地推进了大模型训练的能力。从这个角度出发，微软陆续发布了ZeRO-1，ZeRO-2，ZeRO-3和ZeRO-3 Offload，基本实现了GPU规模和模型性能的线性增长。
大模型的训练数据是大模型的关键要素，其所需的数据的种类也非常广泛，涉及多种模态。以语言大模型为例，其所需要的数据包括多语言数据、代码数据、人工标注数据等多种类别。
大模型的训练数据处理流程和特点根据大模型训练的尺度定律（scaling law），数据规模、模型参数与大模型性能存在紧密关系。近期，微软研究工作表明提高数据质量可以极大地改变尺度定律的形状。通过构建7B的小规模“教科书(Textbooks)”高质量的代码训练数据，训练1.3B模型phi-1在代码评测集HumanEval上Pass@1。
准确率达到了50.6%，超越GPT-3.5（175B，超过2TB训练数据）的47%。该方法表明，通过构建高质量的数据，可以大大降低大模型训练需要的数据规模，具有重要指导意义。下面是几类用于提升数据质量的预处理方法。
质量过滤：语言大模型训练中需要过滤低质量数据，主要分为两类方法：基于分类器的方法和基于启发式的方法。基于分类器的方法是训练一个文本质量判断模型，用以识别并过滤低质量数据。例如，GPT3、PaLM和GLaM模型在训练数据构造时都使用了基于分类器的方法。基于启发式的方法则是通过一组精心设计的规则来消除低质量文本，如BLOOM和Gopher都采用了基于启发式的方法。
冗余去除：语言大模型训练语料库中的重复数据会影响模型性能，降低语言大模型的多样性，并可能导致训练过程不稳定。因此需要对数据进行冗余去除。文本冗余发现是自然语言处理和信息检索中的基础任务之一，可以发现不同粒度上的文本重复，可以有效改善语言模型的训练效果。
隐私消除：预训练数据中可能包含涉及敏感或个人信息，增加隐私泄露的风险。对于此类问题，最直接的方法是采用基于规则的算法删除隐私数据。例如，可以使用基于命名实体识别的算法，检测数据中个人信息内容，并进行删除或者替换。
当前，大模型训练不仅需要大量的无标注数据，而且也需要高质量的人工标注数据，用于模型微调等任务。语言大模型通常需要人类提供明确的指令用于生成有用的输出，标注者通常需要编写提示，典型的提示类型包括如下几种。
普通提示（Plain）：这种类型的提示是为了确保模型的多样性。标注人员需要设计一系列任务，并确保任务具有足够的多样性，以便模型能够了解不同类型的问题和请求。
少量样本提示（Few-shot）：这种类型的提示需要标注人员设计一个指令以及该指令的多个查询/响应对。这些示例应该是常见任务或指令，并且应该涵盖各种不同的主题和情境。
基于用户的提示（User-based）：这种类型的提示需要标注人员根据用户使用案例来编写提示。这些使用案例很有可能是源于用户的实际需要，因此标注人员应该尽可能准确地描述任务和需求。
基于上述收集的数据和提示信息，需要准备三类数据集用于不同训练阶段：SFT数据集、RM数据集、PPO数据集。
在数据构建任务中，随着数据量不断增长，需要开发自动化算法来简化流程。例如，数据增强等环节的自动化受到越来越多的关注。这些任务的自动化不仅会提高效率，而且会提高准确性。此外，自动化可以促进人工标注结果的一致性。
多模态大模型需要有大规模的多模态训练数据，这类数据的收集与处理难度相比于单模态数据更大，需构建以低代价挖掘并实现不同模态之间对齐的高质量多模态数据的方法。未来还需要重点考虑的问题包括：如何构建大模型数据质量评价体系、如何科学地配比训练数据、以及如何在训练不同阶段引入数据等。
当前已经出现一批大模型数据集，涵盖多种模态。代表性的数据集包括ALIGN、VAST-27M、WebVid-2.5M等多模态数据集，以及BookCorpus、Common Crawl、HH-RLHF等语言大模型数据集。
语言大模型预训练数据集：
- BookCorpus：2.24G，包括超过11,000本电子书，涵盖广泛的主题和类型（如小说和传记）。
- OpenWebText：38G，从Reddit上共享的URL中提取的Web内容，且至少获得了3次赞成。
- Common Crawl：PB级规模，一个大型网站抓取数据集，包含原始网页数据、元数据提取和文本提取等内容。
- The Pile：825G，一个大规模、多样化、开源的文本数据集，内容包括书籍、网站、代码、科学论文和社交媒体平台等。
语言大模型指令微调数据集：
- Stanford Alpaca：21.7M，开源的SFT的多样化数据集，包含52,000条指令数据，涵盖创作、生成、设计、代码等多个维度。
- static-hh：90M，开源的SFT多样化数据集，包含100,000条人类对话数据，由LAION、Together、Ontocord.ai这三个机构共同制作。
数据集类型 数据集名称 数据量和简介
话相关大模型训练。
ShareGPT[130] 1.8G，ShareGPT 数据集是一个由用户共享的对话SFT 数据集，包含了超过1 亿条来自不同领域、主题、风格和情感的对话样本，涵盖闲聊、问答、故事、诗歌、歌词等多种类型。
HH-RLHF 120M，Anthropic 创建的大型RLHF 训练数据集，包含161 000 条人工标注的数据。标注人员首先选择自己能够忍受的攻击主题，然后与模型进行对话。
zhihu_rlhf_3k[131] 16M，知乎开源的RLHF 数据集，包含3 000条基于知乎问答的人类偏好数据，包含每个问题中赞同数据较高（chosen）和较低（rejected）的回答，可以用于奖励模型的训练。
BeaverTails[132] 52M，北京大学开源的RLHF 数据集，包含302 000 个数据对，覆盖7 774 个问题。主要标注的方向包含helpful 和harmless 两个维度。
图片-文本多模 SBU[133] 1M，图片-标题对。
数据集类型 数据集名称 数据量和简介
态数据集 COCO [134] 330K，图片/1.5M 标题
Visual Genome[135] 108K，图片-标题对
Conceptual [136] 12M，图片标题对
ALIGN 1.8B，图片-标题对
COYO-700M[137] 747M，图片-标题对
视频-文本多模态数据集 HowTo100M[138] 136M，视频标题对/134 500 小时
WebVid-2.5M 2.5M，视频标题对/13 000 小时
YT-Temporal-180M[139] 1.8M，视频标题对
HD-VILA-100M[140] 100M，视频-标题对
图文音多模态数据集 VALOR-1M[141] 1M，视频-音频-文本数据组
VAST-27M 27M，视频-音频-字幕-文本数据组
随着参数规模和网络结构复杂性的不断提升，大模型开发、训练和推理部署所面临的挑战愈发严峻，其研发依赖算法、算力和数据的综合支撑。深度学习框架及配套工具为大模型的生产和应用提供了基础支撑，涉及开发、训练、压缩、推理和服务等多个环节。此外，通过深度学习框架还可以实现与硬件的适配和协同优化，进一步提升硬件的计算和推理性能，降低大模型开发和应用的成本。
5.1 大模型开发与训练
由于大模型参数规模大，计算和存储的需求显著增加，与辨别式AI模型相比，非常依赖分布式技术提升效率。因此，大模型开发的挑战集中体现在基于深度学习框架对各类分布式并行策略进行本地化配置。为了支持各种分布式并行策略，需要有一套简单、灵活、高效且易于使用的框架和工具界面，使用户可以快捷地进行模型训练和调优，并方便地配置和管理大规模的并行任务。大模型开发也离不开高效的调试工具及方法支撑，非常依赖动态图的调试机制、清晰的调试日志和可视化的调试界面等，帮助开发人员更好地分析模型的行为和表现。
大模型的高性能训练旨在通过对模型计算、显存、内存和通信的系统级优化，在保证模型收敛性的前提下，提高训练吞吐量，实现在有限资源下大模型高效训练的目的。系统级优化方法主要从两个方向实现：一是设备内优化方法，包括降低浮点数的冗余表示的半精度浮点优化、混合精度浮点优化等方法、降低梯度计算过程中冗余表示的梯度检查点（Checkpointing）方法，以及内存优化的ZeRO-Offload方法，即通过将数据和计算从GPU卸载到CPU，以减少神经网络训练期间GPU内存占用的方法。二是多设备优化方法，也称分布式优化，即将分布在不同计算节点上的多个GPU一起用于训练单个模型，这类方法主要有数据并行、张量并行、流水线并
行、分组参数切片并行等多种并行加速策略，下面进行重点介绍。
数据并行[143]：数据并行是每个处理器存储全量的模型参数、梯度和优化器状态，但读取不同的输入数据，在反向计算出参数梯度后，对参数梯度做AllReduce 聚合，然后每个处理器独立进行参数更新。数据并行的优点是实现和使用方式简单，可以通过增加数据并行路数提高训练吞吐，是目前最为常用的分布式并行策略之一。
张量并行[118]：张量并行是将神经网络中同一层的张量运算拆分成多个独立的子运算，并相应地对模型参数做切分，由不同的处理器分别执行，生成的中间结果通过分布式通信进行组合。张量并行的优点是可以充分利用多核处理器的计算能力，减少了内存访问的延迟，但需要设计高效的并行算法和通信机制来确保计算的正确性和高效性，避免通信延迟和带宽瓶颈。
流水线并行[16][144][145]：这种并行策略是将神经网络中的不同层交由不同处理器执行，上下游执行器之间的数据依赖点对点通信传输。基于此技术的高效流水线并行调度策略，支持1F1B、Interleaving 1F1B 等高效调度算法，并通过“通信-计算”重叠的方式隐藏通信时间，提高整体训练效率。
分组参数并行[146]：这种并行策略是一种特殊的数据并行方式，它可以将优化器状态、参数梯度和模型参数切分到不同的处理器上，达到节省大模型显存的目的。分组参数并行的优点是可以有效降低模型显存占用，通过增加数据并行路数提高整体训练吞吐。基于此技术的“组内参数切片+组间数据”并行，可以更合理地分配机内和机间的通信带宽，进一步提升了训练性能。
基于上述基础并行策略，不同深度学习框架的实现方法不同，有的是基于PyTorch 进行进一步封装形成单独的工具，如微软的DeepSpeed-Megatron[147]、NVIDIA 的Megatron-LM[118]、清华大学的BMTrain 等；飞桨PaddePaddle 框架支持四维混合并行技术，可将。
在多维混合并行训练策略的基础上，为了应对模型多样性和训练硬件资源异构性，进一步发展出了端到端自适应分布式训练架构[148]。该架构可以针对不同的深度学习算法抽象成统一的计算视图，自动感知硬件环境并抽象成统一的异构资源视图；采用了代价模型对两者进行联合建模；将模型参数、梯度和优化器状态按照最优策略分配到不同的设备上，构建流水线进行异步高效执行。对于同地域或跨地域多种异构硬件，可以实现节省存储、负载均衡、提升训练性能的目的。此外，针对大模型训练资源不稳定的问题，设计了弹性资源调度管理机制。当资源发生变化时，能够自动的感知硬件环境并修正资源视图，重新触发模型切分放置策略选择及异步流水线执行，使得硬件故障下任务恢复可从小时级降至秒级。
大模型推理往往面临显存占用过多、计算规模庞大、输入输出变长等挑战，这些也是大模型应用落地要重点解决的问题。
在充分考虑大模型结构特性的基础上，可以从模型压缩、推理引擎、服务部署三个关键环节进行全方位的协同优化。这种优化可以在降低时延提升用户体验的同时，最大化提升服务吞吐，实现低时延、高吞吐的效果。
大模型的推理可以通过深度学习框架直接实现，通过框架和模型的协同优化，可以显著提升推理效率。同时，也可以采用专门工具，如FasterTransformer、TensorRT-LLM、vLLM、Text Generation Inference、HuggingFace TG等进行优化，这些工具已经针对大模型推理进行了优化，能够高效地完成任务。推理效率的提升不仅能够改善用户体验，还能显著降低开发成本，有利于大模型在各个领域的广泛应用。
产业界对此非常重视，例如ChatGPT组建了专门的优化团队以降低在线推理成本；百度文心一言通过与飞桨协同优化，推理性能提升了30多倍；腾讯混元大模型通过太极机器学习平台的压缩和分布式推理，资源设备占用减少了40%。
在大模型压缩方面，常规方法包括模型稀疏化、权重矩阵分解、模型参数共享、蒸馏和量化。模型稀疏化通过将模型中的某些神经元、连接或层置为零，实现压缩模型、加速训练、减少内存消耗等目的。权重矩阵分解则使用包括奇异值分解（SVD）在内的矩阵分解方法对预训练模型的Feed-Forward Network（FFN）层的权重矩阵进行分解，减少Attention层的参数量，提高模型效率。
模型参数共享：部分大型模型如ALBERT采用了权重共享的方式，特定层之间共享参数，从而减少了模型的参数量。
蒸馏：通过使用学生模型来模拟预训练教师模型的行为来减小模型大小的技术。通常情况下，学生模型由更小的神经网络或线性模型组成。蒸馏的过程是将教师模型的知识转移到学生模型，使学生模型在保持较小规模的同时，能够保持类似于教师模型的预测能力。
量化：量化是一种将预训练模型中的权重从浮点数转换为低位数的技术。通常情况下，量化的精度可被降低到8位或更低。量化可以大大减少模型的存储空间和计算量，但可能会对模型的性能产生一定的影响。
目前量化技术在大模型压缩时被广泛应用，然而很多量化算法难以做到模型效果无损。自适应Shift-SmoothQuant大模型量化方法可以使激活分布更加平滑，提升量化效果。
对于超大模型精度无损的压缩，可以采用多策略组合压缩方案。通过组合使用模型稀疏化、蒸馏和参数共享等压缩技术，可以在精度无损的情况下，将模型参数量压缩至百分之一、甚至千分之一左右。例如，组合使用低比特量化和模型稀疏化，同时从数值和结构两个维度对大模型的冗余信息进行精简，协同优化计算和访存算子，可以进一步提高压缩率。
大模型推理与服务部署：在推理引擎方面，通用的技术是使用自动计算图融合优化和自动混合并行推理，实现对模型结构和计算硬件的自动感知，协同优化模型推理效率。
子，通过降低算子数量、减少访存次数，获得自动化推理加速能力。
自动混合并行推理：通过自动感知部署硬件的存储、带宽、算力等特性，对模型进行自适应切分，将大模型切分到多个部署硬件上，进行分布式并行推理，尽可能减少卡间通信和跨机通信数据量，从而实现如百亿、千亿参数模型推理部署。
除了上述技术外，推理引擎的优化还可以协同模型压缩，研发符合大模型特点的量化推理方案。例如，语言大模型的上下文计算阶段属于计算密集型，而Token Generation 阶段则属于访存密集型。针对这种计算特点，可以通过协同硬件开展优化，研发LLM.INT8()和Weight Only 量化混合的推理方案。
在服务化调度协同方面，针对生成式模型计算过程中，同一批次输入输出长度不一致带来的计算效率不高问题，通过变长优化降低计算量，并引入输入动态插入批处理技术，可以大幅提升硬件的计算资源利用率，从而提升整体服务的吞吐量。
目前国际上主要的大模型训练芯片有英伟达GPU，如H100、A100，以及谷歌的TPU，国内主要有华为昇腾NPU、昆仑芯XPU、海光DCU、寒武纪MLU 等，其架构和性能规格各不相同。大模型除了对训练芯片的计算性能有一定的要求外，还对硬件的规格，如显存大小、访存带宽和通信带宽具有较高的要求。
为实现大模型的高效训练和推理，需要通过深度学习框架实现与硬件的适配和深度协同优化，通过低成本、高效率的硬件适配方案。
提升大模型与硬件的适配效率，并通过混合精度、显存复用、融合优化等软硬件协同优化技术，结合硬件特性实现系统级优化。
大模型的软硬件适配需要深度学习框架提供标准化的硬件适配开发接口，以对接异构硬件。针对不同AI芯片在指令集、开发语言、加速库、计算图引擎、运行时环境、通信库等方面的差异，需根据AI芯片的技术栈提供差异化的硬件接入方式，涉及算子适配、通信库适配、设备驱动适配等多个方面。
在算子适配方面，有两种方式：
1. 算子映射：框架算子库对接硬件算子库，提供单算子粒度级别的接入方式，交由框架执行器进行算子库接口的调用和执行，适用于底层硬件SDK支持硬件算子库。
2. 算子开发：芯片厂商在其软件栈提供高级开发语言，如NVIDIA的CUDA C开发语言，深度学习框架通过高级开发语言实现算子代码的开发。
神经网络编译器接入通过深度学习框架中的神经网络编译器中间表示（Intermediate Representation，IR）对接硬件的代码生成器（Codegen），提供编译器IR到底层硬件IR的转化，交由编译器进行算子融合和调度，适用于底层硬件SDK支持代码生成的硬件。
大模型的软硬件协同优化在显存优化、计算加速和通信优化三个环节需要提供相应的优化技术。在显存优化方面，框架支持多层显存复用、重计算和低比特量化等技术，降低大模型对硬件显存的要求；在计算加速方面，框架支持混合精度、算子融合优化等技术，并通过接入硬件Transformer大算子库，针对生成式大模型进行深度融合优化，提升大模型性能；在通信优化方面，...（此处内容截断，未提供完整句子，故不予以保留）。
框架支持自适应的通信拓扑优化技术，可感知硬件集群环境的配置，搜索最优并行策略，支持大模型在不同规模集群下的高效训练，提升模型性能的同时，降低开发者配置高效大模型训练的门槛。硬件加速是大模型高效计算的另一种关键技术，通过使用专用硬件来优化神经网络计算，以达到更高的性能和效率。例如，TPU专门为深度学习计算进行了定制化优化，ASIC加速是另一种硬件加速的方案，它是一种定制化的集成电路，专门为某个特定应用场景而设计制造。FPGA加速也是一种重要的硬件加速技术，具有高度灵活性和可编程性。云服务为大模型训练提供了强大的计算能力和存储资源，可以根据用户的实际需求和流量变化，灵活调整计算资源的规模和配置。综上，大模型对软硬件协同优化提出了更好的要求，一方面需要对现有硬件进行全面适配，另一方面需要开展极致的软硬件协同优化，才能有效支撑大模型的研发和广泛应用。
大模型应用
大模型因其强大的自然语言与多模态信息处理能力，能够应对不同语义粒度下的任务，进行复杂的逻辑推理，还具有超强的迁移学习和少样本学习能力。这些特点使其能够快速掌握新任务，实现不同领域、不同数据模式的适配，容易赋能其他行业，提升行业效率。在信息检索领域，大模型能从用户问句中提取查询意图，检索出符合用户意图的结果，改写查询语句以检索到更相关的信息。在新闻媒体领域，大模型可以自动化生成标题、摘要、正文等。
信息检索
近年来，搜索引擎提供的功能逐步丰富，但仍然基于经典的检索范式：基于关键词的用户查询，高效地从海量文档中检索相关文档并排序返回。检索系统通常分为离线和在线两个阶段。离线阶段预处理文档并构建索引，包括早期的倒排索引和近年来的向量索引。在线阶段，检索系统理解用户查询后，通过检索模型计算文档和查询的相关性，召回最相关的TopK候选文档，再通过性能更强的精排模型进行排序输出。这种“索引—召回—精排”的检索架构广泛应用于各种信息检索系统中。
以从海量的互联网内容中获取准确的信息，但是对检索结果通常不做深入分析。当用户信息需求比较复杂时，需要用户浏览多个结果才能获取所需的信息。而生成式大模型则是将大量知识存储在参数化的模型中，可以直接根据用户的问题生成答案，能够更便捷地满足用户的信息需求。将两种信息获取范式的优势进行融合与互补，打造更为高效、准确的信息获取技术，具有重要的科学价值与应用意义。
6.2 新闻媒体
中国科学院自动化研究所基于自主研发的音视频理解大模型“闻海”和三模态预训练模型“紫东太初”，联合新华社媒体大数据和业务场景，在2021年12月推出了“全媒体多模态大模型”。该项目通过构建大模型……
数据与大模型驱动的多任务统一学习体系，实现了对全媒体数据的统一建模和理解生成。该模型兼具语音、图像、文本等跨模态理解和生成能力。项目将加速AI技术在视频配音、语音播报、标题生成、海报设计等多元业务场景中的应用。
在智慧城市方面，阿里巴巴的多模态大模型M6已被应用于Talk2Car任务中。具体地，用户通过给出一个指令，比如“在前面那个绿车前面停下来”，就可以定位指令中所指的车辆。
2023年7月7日，城市大模型CityGPT正式发布，旨在提升智能城市的治理能力，赋能城市经济、产业、商业、文旅、金融等领域，打造真正的城市级大脑。具体地，在认知人工智能领域首次开启了空间场景智能决策以及“元宇宙城市”可交互体验价值链，能够实现对城市-园区-商圈-社区-网点级别的智能计算与研判，为线上线下数实融合的智能决策和场景交互提供具有AI自学习能力的“空间AI专家顾问”服务。
DNA远端交互进行基因表达和染色质状态预测的神经网络架构Enformer，能够一次编码超过20万个碱基对，大幅提高了根据DNA序列预测基因表达的准确性。美国哈佛医学院和英国牛津大学的研究人员合作开发出一款可准确预测致病基因突变的AI模型“EVE”，已预测出3200多个疾病相关基因中的3600万个致病突变，并对26.6万个至今意义不明的基因突变进行了“致病”还是“良性”的归类。AlphaFold2通过深度学习和人工神经网络等技术，预测蛋白质的三维结构。微软推出的新一代办公软件Copilot，将大模型应用于办公场景，实现智能化协助用户提高工作效率。在文字处理软件Word中，Copilot可以协助用户撰写各类文档，实现文档创作、编辑和总结等功能。在演示文稿软件PowerPoint中，Copilot可以根据用户的要求，自动...（由于最后一句话存在截断，故在此省略）。
生成演示文稿幻灯片。在电子表格软件Excel中，Copilot可以完成数据统计分析，并将结果以图表的形式清晰可视化呈现。
在影视行业，大模型技术为内容制作和影视创作带来了新的变革。大模型可以应用于剧本创作、角色设计和音乐配乐，为影视制作带来更多元化和个性化的创意。此外，大模型还能用于视频内容分析，实现内容标签化和智能推荐，提升观众的观影体验。
智能教育方面，2023年，国内教育科技公司积极布局教育领域大模型，推出多项创新应用，以智能化手段提升教与学效果。网易有道发布面向K12教育的大模型“子曰”，实现个性化分析指导、引导式学习等功能，大模型能够较好地因材施教，为学生提供全方位知识支持。好未来发布数学领域大模型MathGPT，可自动出题并给出解答，涵盖小学到高中数学知识。教育领域大模型正成为智能辅助教学的新工具。
整合能力可满足学生动态需求，实现个性化学习，与教师共同提高教学质量。
智慧金融
2023年6月，恒生电子发布多款大模型金融应用，其中金融行业大模型LightGPT使用超过4000亿字节的金融领域数据进行预训练，支持80多项金融专属任务，能准确理解金融业务场景需求。8月，马上金融发布国内首个零售金融大模型“天镜”，具有知识汇集、唤醒数据价值等应用场景，可助力零售金融机构实现智能客服、精准营销、风险控制等能力。在模型训练规模不断扩大的背景下，金融行业大模型精度持续提升，已经成为金融机构实现业务智能化的重要途径。
智慧医疗
2023年5月，医联推出医疗语言模型MedGPT，实现从预防到康复的全流程智能诊疗，提升实际临床应用价值。7月，谷歌DeepMind研发Med-PaLM医疗大模型，其在医学考试和开放式问答上达到专家水平，回答准确率高达86.5%，大幅超过早期版本。非专业评估者也高度认可其问诊效果。同月，京东健康发布“京医千询”大模型，可以理解医学多模态数据，并根据个性化诊疗需求进行智能决策。医疗大模型正在成为提升临床决策效率和服务水平的重要工具，通过学习处理海量医学知识，可以高效辅助各环节工作，具有广阔的应用前景。
智慧工厂
服饰行业中，阿里巴巴开发的多模态大模型M6已成功应用于犀牛新制造，实现了例如文本到图像生成等多种应用案例。传统服装设计过程中，设计师需要花费很长的时间设计衣服并进行线上样款测试，但基于文本到图像生成技术，可以直接输入流行的服装款式描述到M6模型中生成相应款式图片。这项技术将原本冗长的设计流程压缩了超过十倍的时间，目前已经商业投产，并且与三十多家服装商家在双十一期间成功地进行了合作。
阿里巴巴的多模态大模型M6已经在众多民生服务领域产生影响。M6除了提供文本到图像生成的能力，还可以根据交互需求不断完善生成结果。例如，在给定一张衣服图像时，用户可以保留其领子并进一步个性化调整。M6每次可以只生成一部分的token，随着多次迭代，生成结果越来越好。M6还被用于生成营销文案，只需要使用原来5%左右的样本，即可实现百分之八十五以上的通过率。这得益于多模态预训练，即输入不仅包括文本，还可以输入图像，大大增加了模型的预测效率。M6模型还被应用于生成推荐理由，并已在阿里小蜜上线。在数字人应用中，如淘宝直播，通常需要使用语音识别（ASR）将主播的口述转换为文本形式，借助于多模态深度学习模型M6，这一过程已经成功实现。
华为云盘古大模型研发团队的研究成果在级学术期刊《自然》杂志正刊发表。该模型使用了39年的全球再分析天气数据进行训练，其预测准确率与全球最佳数值天气预报系统IFS相当，同时，在相同的空间分辨率下，速度提升了10000倍以上，并保持了极高的精准度。
此外，大模型的应用场景广泛，包括但不限于智能创意领域，如游戏、广告、美术和影视，可帮助实现角色立绘、特效设计、动画分镜等，显著提升工作效率，降低制作成本。在自动驾驶领域，通过融合视觉、雷达、红外等多模态传感器数据，实现对道路、车辆和行人的全方位感知和理解，推动技术发展。同时，它还为智能辅助设备如智能助理、智能家居提供更自然智能的人机交互方式，以提升用户体验。
大模型的安全性
第7章 大模型安全风险引发全球广泛关注
与大模型技术的突飞猛进形成鲜明对照的是，大模型仍面临诸多潜在的安全风险。大模型在应用的过程中，可能会产生与人类价值观不一致的输出，如歧视言论、辱骂、违背伦理道德的内容等，这种潜在的安全风险普遍存在于文本、图像、语音和视频等诸多应用场景中，并会随着模型的大规模部署带来日益严重的安全隐患，使得用户无法信赖人工智能系统做出的决策。更为重要的是，大模型较为脆弱，对安全风险的防范能力不足，容易受到指令攻击、提示注入和后门攻击等恶意攻击。尤其是在政治、军事、金融、医疗等关键的涉密应用领域，任何形式的恶意攻击都可能给国家社会的稳定以及人民的生命财产安全带来严重的后果。
2023年4月28日，
为确保大模型的安全和负责任地使用，各国的监管机构都在积极探讨并制定相应的安全标准和准则，为开发者和企业提供清晰的大模型应用和治理方向。
2021年11月，联合国教科文组织正式发布《人工智能伦理问题建议书》，指出可以引导人工智能技术向着负责任的方向发展。
2023年3月，美国白宫科技政策办公室发布《促进隐私保护数据共享和分析的国家战略》。该策略旨在保障公共和私营部门实体中用户的数据隐私，同时确保数据使用的公平性和最大的效率。
2023年4月，美国政府发布《人工智能问责政策征求意见》，征求意见稿涵盖人工智能审计、安全风险评估、认证等内容，以促进建立合法、有效、合乎道德、安全可信的人工智能系统。
2023年6月，欧洲议会通过《人工智能法案》草案，旨在为人工智能引入统一的监管和法律框架，并涵盖了除军事用途外的所有人工智能类型。
作为AI技术的重要发展地之一，中国非常重视人工智能和大模型的安全监管。习近平总书记在多次会议中指出，“要重视
智能发展，营造创新生态，重视防范风险”，“要加强人工智能发展的潜在风险研判和防范，维护人民利益和国家安全，确保人工智能安全、可靠、可控”。国内相关机构积极制定大模型发展的安全规范。
2019年6月，国家新一代人工智能治理专业委员会发布的《新一代人工智能治理原则——发展负责任的人工智能》指出，“人工智能系统应不断提升透明性、可解释性、可靠性、可控性，逐步实现可审核、可监督、可追溯、可信赖。高度关注人工智能系统的安全，提高人工智能鲁棒性及抗干扰性，形成人工智能安全评估和管控能力。”
2020年7月，国家标准化管理委员会、中央网信办、国家发展和改革委员会、科学技术部、工业和信息化部发布的《国家新一代人工智能标准体系建设指南》指出，“重点开展人工智能安全术语、人工智能安全参考框架、人工智能基本安全原则和要求等标准的研制”。
2021年9月，国家新一代人工智能治理专业委员会发布《新一代人工智能伦理规范》，旨在“将伦理道德融入人工智能全生命周期，促进公平、公正、和谐、安全，避免偏见、歧视、隐私和信息泄露等问题。”
2022年3月，中共中央办公厅、国务院办公厅发布的《关于加强科技伦理治理的意见》指出，应“加快构建中国特色科技伦理体系，健全多方参与、协同共治的科技伦理治理体制机制，坚持促进创新与防范风险相统一、制度规范与自我约束相结合，强化底线思维和风险意识，建立完善符合我国国情、与国际接轨的科技伦理制度，塑造科技向善的文化理念和保障机制”。
2023年3月，国家人工智能标准化总体组、全国信标委人工智能分委会发布《人工智能伦理治理标准化指南》，明确了人工智能伦理治理概念范畴，细化人工智能伦理准则内涵外延，对人工智能伦理风险进行分类分级分析，提出人工智能伦理治理技术框架，构建人工智能伦理治理标准体系，引导人工智能伦理治理工作健康发展。
2023年7月，国家互联网信息办公室、国家发展和改革委员会等发布的《生成式人工智能服务管理暂行办法》指出，“国家坚持发展和安全并重、促进创新和依法治理相结合的原则，采取有效措施鼓励生成式人工智能创新发展，对生成式人工智能服务实行包容审慎和分类分级监管”、“提供和使用生成式人工智能服务，应当遵守法律、行政法规，尊重社会公德和伦理道德”。
大模型安全风险的具体表现随着大模型在各领域的广泛应用，其影响范围逐渐扩大，社会秩序收到的冲击愈发严重。其安全风险可以从大模型自身的安全风险、以及大模型在应用中衍生的安全风险两个方面进行细致地分析。
大模型自身的安全风险源于其开发技术与实现方式。这些模型通常采用大量数据进行训练，它们不仅从数据中学习知识和信息，还可能从中吸收和反映数据中存在的不当、偏见或歧视性内容。这些数据可能来源于互联网或其他公开来源，其中包含的多样性和复杂性导致模型很难完全准确地反映人类的价值观和伦理标准。大模型在处理或生成内容时，可能会无意中扩大或放大某些固有的社会偏见。
典型的风险类型包括：
（1）辱骂仇恨：模型生成带有辱骂、脏字脏话、仇恨言论等不当内容。
（2）偏见歧视：模型生成对个人或群体的偏见和歧视性内容，通常与种族、性别、宗教、外貌等因素有关。
（3）违法犯罪：模型生成的内容涉及到违法、犯罪的观点、行为或动机，包括怂恿犯罪、诈骗、造谣等内容。
敏感话题：模型可能输出具有偏向、误导性和不准确的信息，如支持特定政治立场而歧视或排斥其他观点。
身体伤害：模型可能生成不安全的健康信息，如误导性医学信息或错误的药品使用建议，对用户健康造成风险。
心理伤害：模型可能输出影响心理健康的信息，如鼓励自杀、引发恐慌或焦虑等内容。
隐私财产：模型在处理涉及用户或第三方隐私和财产信息时，应遵循相关法律和隐私规定，避免信息泄露和滥用。
伦理道德：模型生成内容时需遵循伦理原则和道德规范，与人类价值观保持一致。
语言模型的意识形态是AI安全的核心考量因素。模型受训练数据中的文化与价值观影响，可能偏向特定价值观。为确保准确传递文化价值观，应深化安全对齐技术，并根据不同文化背景调整模型意识形态。
大模型应用中衍生的安全风险包括不当使用和恶意使用等行为。用户过度依赖模型的生成内容可能导致“幻觉”问题，即看似真实却错误的输出，影响决策。
在医学诊断、法律意见等需要高精度的领域，盲目信赖会带来巨大风险。大模型面临着模型窃取攻击、数据重构攻击、指令攻击等多种恶意攻击。模型窃取攻击允许攻击者获取模型的结构和关键参数，可能带来其他利益。数据重构攻击能恢复模型的训练数据，对个人隐私和数据所有权构成威胁。指令攻击则利用模型对措辞的敏感性，诱导其产生违规或偏见内容，违反原安全设定。
后门攻击是一种针对深度学习模型的新型攻击方式，其在训练过程中植入隐秘后门。后门未被激活时，模型可正常工作，一旦被激活，则输出攻击者预设的恶意标签。此外，后门攻击具有可迁移性，为攻击者创造新的攻击途径。
大模型与外部数据、API或其他敏感系统的交互涉及诸多安全挑战。当大模型从外部资源获取信息时，若连接未经适当安全措施保护，会导致模型生成不安全和不可靠的反馈。自主智能体AutoGPT的例子显示了在缺乏人工监管时可能出现的无法预测的行为模式。因此，对于大模型与外部资源的交互，需要特别关注并采取严格的安全策略。
随着大模型安全问题的日益凸显，全球众多知名的科研机构已将此作为核心研究领域，致力于探索模型的潜在薄弱点和安全风险，并寻求如何增强其在训练和部署时的安全性。
7.4.1 大模型的安全对齐技术
安全对齐的大模型通常是指经过充分检验、具备高可信度和鲁棒性、与人类价值观对齐的大型机器学习模型。这些模型的设计和训练过程严格遵循伦理准则，具备透明度、可解释性和可审计性，使用户能够理解其行为和决策过程。同时，安全对齐大模型也需注重隐私和安全，确保在使用过程中不会泄露敏感信息或被恶意攻击。
大模型暴露的安全风险，与其开发技术密不可分。当下主流的大模型训练过程可分为预训练、有监督微调和基于反馈的强化学习微调三个阶段。
针对大模型开发过程中产生的安全风险，安全对齐研究可从提升训练数据的安全性、优化安全对齐训练算法两个方面展开，以实现更有用、诚实和无害的安全大模型。
（1）大模型的训练数据安全
训练数据的安全性是构建安全大模型的基石。训练数据安全是指数据集的来源和质量都是可靠的，数据中蕴含的知识是准确的，数据集内容符合主流价值观。以下是提高数据安全性的一些关键要点：
数据的来源与预处理。确保训练数据来自可信的、可靠的来源。数据应该从权威机构、专业组织、可验证的数据仓库或其他公认的数据提供者获得。在数据标注时，确保标注的准确性和一致性。标注过程应该由经过培训的专业人员进行，并且需要进行验证和审核，以确保标注的正确性。此外，需要进行数据清洗以去除重复项、噪声数据和错误数据。
数据的敏感信息去除。在大模型中，保护数据的敏感信息至关重要，特别是当模型需要处理涉及个人隐私、敏感信息或商业机密等敏感数据时。数据的敏感信息去除是一种隐私保护措施，旨在确保数据在训练过程中不会泄露敏感信息。常见的数据的敏感信息去除方法有以下几种：
a. 数据脱敏（Data Anonymization）：数据脱敏是一种常见的敏感信息去除方法，它可以通过不同的技术手段对数据进行处理，以确保数据中的敏感信息无法被还原或追溯到特定个体。常见的脱敏方法包括随机化、泛化、替换和加噪声等。
b. 去标识化（De-identification）：去标识化是指删除数据中的个人标识信息，例如姓名、地址、身份证号码等，从而将数据匿名化。这样可以确保数据无法直接与特定个体关联。
c. 数据掩码（Data Masking）：数据掩码是一种将敏感信息部分替换为伪造或不可还原的数据，从而确保原始敏感信息无法被还原的方法。
在进行数据的敏感信息去除时，需要谨慎处理，以确保不会破坏数据的完整性和质量。同时，也需要注意确保去除敏感信息后的数据仍然具有足够的信息量和代表性，以确保训练的模型具备合理的性能和泛化能力。
大模型的安全对齐训练基于反馈的安全对齐技术。基于人类反馈的安全对齐技术已逐渐…[此处文本截断，未提供完整内容]
成为当下大模型安全研究的主流技术，其训练过程主要包括奖励模型训练和生成策略优化两个子阶段。在奖励模型训练阶段，人类对模型生成的多条不同回复进行评估，生成的人类偏好标签使奖励模型能学习并拟合人类的偏好。在生成策略优化阶段，奖励模型根据生成回复的质量计算奖励，这个奖励作为强化学习框架中的反馈，用于更新当前策略的模型参数，从而让模型的输出更符合人类的期望。DeepMind 使用RLHF 技术构建更有用、更准确和更安全的对话智能体Sparrow。Anthropic 公司提出的Claude 模型则采用了RLAIF技术，减少对人类反馈的依赖。北京大学团队开源了名为 PKU-Beaver的项目，复旦大学发布了基于RLHF 实现人类对齐的MOSS-RLHF 模型，深入探究了RLHF阶段所采用的强化学习算法PPO，并发布技术报告与开源核心代码。
大模型可信增强技术，在训练过程中，模型可通过对抗训练和知识融入训练增加可信度。对抗训练通过提升模型对输入扰动的鲁棒性，减少对抗性攻击的影响，并提升大模型的泛化能力。知识融入训练则是利用知识引导模型训练，降低模型出现幻觉的可能性，结合知识图谱的模型训练是典型的知识融入训练方法。
知识图谱中的三元组加入到模型的训练过程中，用三元组中的知识引导模型的训练，促使大模型沿着具有正确知识的方向收敛，从而让大模型存储到高可信度的知识。
大模型安全性评测技术是大模型安全发展的有力保障。为了评估大语言模型的安全性，并推动安全、负责任和合乎道德的人工智能的发展和部署，清华大学于2023年3月推出面向中文大模型的安全性评测平台。该平台依托于一套系统的安全评测框架，从辱骂仇恨、偏见歧视、违法犯罪等八个典型安全场景和六种指令攻击综合评估大语言模型的安全性能。其中，指令攻击是指一般模型难以处理的安全攻击方式，这些攻击更容易诱导模型出错，包含目标劫持、Prompt 泄露、赋予特殊的角色后发布指令、不安全/不合理的指令主题、隐含不安全观点的询问，以及反面诱导。
平台已开源大模型安全评测的数据基准，并测试了包括ChatGPT在内的十余个主流大模型，其安全分数以排行榜的形式在平台公布。
大模型极端风险的评估。随着AI 技术的进步，大模型将会显示出更多危险的突发能力，如进行攻击性的网络操作、通过对话操纵人。
为了识别这些风险，DeepMind 联合OpenAI、Anthropic 等单位提出针对新型威胁评估的通用模型框架，认为大模型安全评估首先应评估模型是否具有某些危险的能力，其次判断模型多大程度上可能使用这些能力造成伤害。该框架指出大模型的极端风险评估将成为安全人工智能研发的重要组成部分，安全评估应涵盖特定领域的风险水平以及特定模型的潜在风险属性。极端风险评估可以帮助开发者识别可能导致极端风险的因素，并为模型训练和部署过程中的安全性优化提供参考。
大模型行为决策的道德评估。随着AI 系统能力的快速增长，越来越多的大模型被训练应用于真实世界的交互任务。为了衡量大模型在各种社会决策场景中的能力和道德行为，一项典型的评测基准是MACHIAVELLI。它主要由134款基于文本的Choose Your Own Adventure 游戏组成，在评估中为大模型代理提供真实世界的目标，并通过专注于高层次的决策来追踪代理的不道德行为，以评估其在现实社会环境中的规划能力及安全风险。该项研究发现，道德行为和最大化奖励之间存在权衡的关系，但通过设计道德提示，对大模型进行道德调节，可缓解权衡、并降低有害行为的频率。
```
图7-3 道德行为评测基准MACHIAVELLI
```
近年来，大模型技术飞速发展，从架构演进统一到训练方式转变，再到模型高效适配，引起机器学习范式的一系列重要革新，为通用人工智能发展提供了一种新的手段。大模型技术融合多种模态信息，实现多模态感知与统一表示，也将和知识图谱、搜索引擎、博弈对抗、脑认知等技术融合发展，相互促进，朝着更高智能水平和更加通用性方向发展。
与此同时，大模型技术生态蓬勃发展，开源服务与开放生态成为主流趋势。国内外大模型开放平台、开源模型、框架、工具与公开数据集加速大模型技术演进，框架、工具间软硬件协同优化降低大模型开发和应用成本，推动大模型高效训练与部署。
大模型与教育、科学、金融、传媒艺术等专用领域结合拓广通用大模型能力边界，与实体经济的深度融合成为其赋能行业应用关键，正在“大模型”与“小模型”端云协同并进发展格局下重塑生产力工具，变革信息获取方式，改变人类社会生活和生产方式。
随着大模型的应用，其安全问题日益凸显，因而需关注大模型技术发展的内生及伴生风险，关注大模型安全对齐、安全评估技术，发展大模型安全增强技术，加强大模型安全监管措施，确保其“安全、可靠、可控”。
总之，抓紧推动大模型技术研发，尤其是大模型原始技术创新和大模型软硬件生态建设，强化垂直行业数据基础优势，集中国家资源投入大模型发展，同时关注大模型风险监督，彰显人工智能的技术属性和社会属性。
8.1 协同多方合作，共同推动大模型发展
加强学术界和企业界之间合作，是推动大模型生态安全健康发展的重要方面。为了促进校企之间的合作，政府可鼓励建立学术界和企业界之间的合作平台，以促进知识共享和技术交流。包括设立联合研究中心、实验室或合作项目，为学术研究人员和企业工程师提供合作机会和资源。其次，政府可推动学术界和企业界之间的数据共享和协同研究，以增进对大模型训练数据的理解和分析。共享数据可帮助学术界更好地理解大模型的特性和潜在风险，而企业界可受益于学术界的深入研究和分析，进一步改进算法和模型的安全性。此外，应促进人才培养和交流。通过设立奖学金、建立博士生联合培养计划、鼓励学术界研究人员在企业界进行实地访问等方式，促进校企之间的人才培养和交流，培养具备学术和实践经验的人才，推动大模型安全可持续发展。
在大模型训练过程中，算力紧缺成为一个重要挑战。为应对算力紧缺问题，首先，政府部门可推进建立云计算平台，提供强大算力资源和相应服务，以支持大型模型的安全训练和推理。其次，政府部门可推动产业和学术界之间的合作，共享算力资源。通过建立合作机制和共享平台，不同实体可共同利用算力资源，减轻各方算力压力。政府可支持推动分布式计算技术的研究与创新。分布式计算技术可将多台计算机或服务器连接在一起，形成计算集群，从而提供更大规模的计算能力。最后，政府可制定激励政策，鼓励企业和研究机构投资和发展与大模型算力相关的技术和设施，包括提供税收优惠、资金支持、知识产权保护等方面的激励措施，以吸引更多的投资和创新。
相关部门可牵头制定人工智能的合规标准和开发指南，全面覆盖大模型的研发、训练和部署过程中的安全要求和最佳实践，以及对大模型的能力水平进行评估的方法。这样的举措有助于企业和研究机构建立健全的治理机制和风险管理体系，推动行业的规范化发展。通过制定合规标准，可以确保大模型的研发过程符合道德和法律要求，包括数据采集和使用的透明度和合法性，隐私保护措施，以及对敏感主题和内容的处理原则。开发指南提供训练和部署大型模型时的最佳实践参考，以辅助提升模型可靠性、鲁棒性和公平性。
通过制定大模型能力水平的评测标准和方法，可衡量其在不同任务和领域的表现，帮助用户和开发者更好地了解和评估大型模型的性能和可靠性，为其选择合适的应用场景提供参考。评测平台可提供标准化的评测数据集、评估指标和基准结果，以促进模型性能的客观比较和提升。平台应包含多样化的评测任务，涵盖自然语言处理理解、文本生成、代码生成、安全伦理等不同领域和应用。
此外，应制定一套针对中文背景下大模型评测的规范和方法论，明确评测过程中的数据准备、评估指标、测试方法等细节。这有助于保证评测的可重复性和公正性，并提供统一标准来衡量不同模型的性能和效果。
制定大模型合规标准、建立中文大模型评测平台，将有助于提供公正、可靠的评测环境，推动中文大模型技术发展和应用。同时，评测平台也为学术界、企业界和开发者提供交流和合作平台，促进创新和协同发展。
此外，可制定大模型发展纲要，在大模型核心环节和相关技术上做知识产权布局。在应用生态上，建议组建包括由芯片、云计算、互联网、应用等上下游企业组成的产业发展联盟，鼓励相关企业基于大模型进行数字化转型升级，支持产学研三方协同的大模型研发模式。
应对大模型带来的安全性挑战，大模型存在大量安全漏洞，迫切需要加大力度进行大模型鲁棒性检测与防御技术研发，并重视大模型对网络安全的影响。德国萨尔大学指出，现有语言大模型易受对抗性攻击，可通过自然语言提示实现灵活调节。全新的攻击媒介使用间接性提示注入，能让攻击者远程利用集成大模型的应用。从计算机安全角度出发，需要设计系统的分类法以研究集成大模型的应用中的潜在漏洞，探讨攻击的传递方式及可能造成的威胁。
一系列实验表明，简单的提示即可控制模型行为，当前过滤技术似乎无法防范这种间接提示注入。随着大模型功能增强，几乎可将所有已知网络安全威胁转移到新的大模型生态系统中，对大模型潜在应用部署造成隐患。因此，应关注新出现的潜在漏洞，推动大模型相关应用更鲁棒与安全的部署。
同时，应重视大模型对网络安全的影响。虽然Deepfake算法如GAN能生成逼真虚假内容，但控制手段如ChatGPT的引入可减少不良内容产生。网络安全管理者担心大模型被黑客滥用的风险。降低大模型对网络安全不良影响的措施包括：网络检测和响应，研究全面的解决方案来监控潜在风险活动；密码安全和防护，确保高强度密码的独特性和难以破译性；双因素身份验证。
使用双重认证（2FA）也可增强网络安全性。用户除了输入密码外，还必须输入发送到其手机或电子邮件中的验证码；第四，软件更新，保持操作系统和其他程序的更新，确保其采用最新补丁；第五，杀毒软件，确保手机和设备安装杀毒软件防范在线机器人。
鼓励企事业单位使用国产深度学习框架开展大模型训练和推理，加强大模型构建所需基础软件的自主可控性；引导国产芯片厂商基于国产框架开展与大模型的适配和融合优化，打造功能完备的国产人工智能基础设施，推动大模型技术栈自主可控。